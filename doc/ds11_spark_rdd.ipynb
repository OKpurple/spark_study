{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark RDD\n",
    "\n",
    "* Last updated 20170515 20170221 20161125\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* 파일, JSON 등에서 RDD를 생성할 수 있다.\n",
    "* RDD API를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.2 Data를 사용하는 API \n",
    "* S.3 SparkSession\n",
    "* S.3.1 Spark 2.0 vs 1.6 \n",
    "* S.3.2 SparkContext\n",
    "* S.3.3 IPython Notebook에서 SparkSession 생성하기\n",
    "* S.4 RDD 소개\n",
    "* S.5 RDD 생성\n",
    "* S.6 RDD API \n",
    "* S.6.1 비슷한 Python 함수\n",
    "* S.6.2 RDD 사용하기\n",
    "* S.7 spark-submit\n",
    "\n",
    "### S.1.3 문제\n",
    "\n",
    "* 문제 S-1: Hello Spark - 환경설정을 읽어 클라이언트를 생성하기.\n",
    "* 문제 S-2: RDD를 사용하여 word count를 계산하고 그래프 그리기.\n",
    "* 문제 S-3: RDD를 사용하여 word vector를 생성하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 데이터를 사용하는 API\n",
    "\n",
    "* Spark는 RDD, Dataframe, DataSet으로 구분하여, 데이터구조를 사용한다.\n",
    "* 이 중 RDD는 버전 1.0부터 사용되었고, 다른 데이터구조는 RDD를 기반으로 만들어졌다.\n",
    "* 현재 RDD API에 대한 지원은 축소되고 있다.\n",
    "\n",
    "데이터구조 | 도입된 spark version | 설명\n",
    "---------|---------|---------\n",
    "RDD | Spark 1.0 | unstructured, schema free, low-level\n",
    "Dataframe | 1.3 | semi 또는 structured, schema를 가진다. Dataset[Row]와 같은 의미로, 타잎을 강제하지 않는다.\n",
    "Dataset | 1.6 | 자바의 Generic과 같이 Dataset[T]으로 '타잎'을 강제하는 형식이다. Scala and Java에서 사용한다. Python loosely-typed이므로 사용하지 않는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* RDD는 데이터가 비구조적인 경우 사용하기 적합하다. 모델schema를 정하지 않고 사용할 수 있다.\n",
    "* RDD는 하위수준의 API를 제공하고, 다른 데이터구조는 schema 및 데이터타잎을 지원하게 된다.\n",
    "* DataFrame과 DataSet은 데이터가 구조적인 경우 사용한다.\n",
    "* Spark의 RDD, DataFrame 모두 immutable\n",
    "* Spark의 데이터는 모두 lazy (실제 transformation을 action까지 연기)\n",
    "* Spark에서 모든 transformations이 연기된다는 lazy의 의미는:\n",
    "    * 변환을 하여 메모리에 가지고 있는 비효율성\n",
    "    * 실제 action이 실행되는 경우, 계산이 이루어지고, 실제 메모리를 사용한다.\n",
    "    * RDD의 경우, action이 실행될 때마다 재계산이 이루어지는 것을 막기 위해 persist (or cache)함수를 사용할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 SparkSession\n",
    "\n",
    "### S.3.1 Spark 2.0 vs 1.6\n",
    "\n",
    "* Spark 2.0부터는 SparkSession으로 시작점을 통합해서, 개별적인 Context를 모두 통합해서 'pyspark.sql.SparkSession'를 사용한다.\n",
    "\n",
    "```\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "Context 구분 | 설명 | 2.0의 사용 예 | 1.x의 사용 예\n",
    "----------|----------|----------|----------\n",
    "SparkContext | RDD를 사용하는 Context | spark.SparkContext | SparkContext()\n",
    "StreamingContext | 향후 제공 | |\n",
    "SQLContext | Spark SQL, DataFrame | spark.sql | SQLContext(SparkContext)\n",
    "HiveContext | HiveQL, DataFrame | spark.sql | HiveContext(SparkContext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 이전 1.x 버전에서는 SparkContext를 통해 다른 Context를 사용했고, 호환성을 제공하므로 그대로 사용할 수도 있다.\n",
    "* Spark 1.6에서는 다음과 같이 사용한다.\n",
    "\n",
    "```\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "sc = pyspark.SparkContext(conf=conf) #SparkContext를 직접 생성한다.\n",
    "sqlContext = SQLContext(sc)   #SparkContext를 넣어서 SQLContext를 생성\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.2 SparkContext\n",
    "\n",
    "* Spark 서버(클러스터)에 대한 클라이언트와 같은 역할을 한다.\n",
    "* 클러스터를 어떻게 사용할 것인지 정한다. cluster manager에서 system resource를 할당받는다 (cpu, memory, machine)\n",
    "* Python의 SparkContext는 jar를 분산환경에서 사용하게 되므로 주의 (Scala, Java와 다름)\n",
    "    * pyFiles에 사용할 (의존적인) 라이브러리를 넣는다 (또는 사용할 라이브러리가 없으면 빈 파일로 둔다).\n",
    "\n",
    "* Cannot run multiple SparkContexts at once;\n",
    "    * sc가 이미 있는 경우 sc.stop()\n",
    "\n",
    "```\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"master\",\"my python app\", sparkHome=\"sparkhome\",pyFiles=\"placeholderdeps.zip\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.3 IPython Notebook에서 SparkSession 생성하기\n",
    "\n",
    "* Spark에서 pyspark shell을 제공하고 있다.\n",
    "* 이 강의에서는 ipython notebook에서 pyspark를 사용하기로 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: Hello Spark, 환경설정을 읽어 클라이언트 생성하기.\n",
    "\n",
    "* 설정을 변경한다.\n",
    "\n",
    "설정 항목 | 설명\n",
    "----------|----------\n",
    "SPARK_HOME | Spark가 설치된 경로로 수정한다.\n",
    "PYTHONPATH | sys.path.insert()를 사용하여 'PYTHONPATH'를 수정한다. pyspark.zip, py4j-0.10.1-src.zip를 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 윈도우\n",
    "    * 'HOME'이 설정되어 있지 않으므로, expanduser('~')를 사용한다.\n",
    "```\n",
    "os.path.expanduser(\"~\")\n",
    "```\n",
    "\n",
    "    * Backslash Continuation은 사용하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip\n",
      "\n",
      "/usr/lib/python2.7\n",
      "/usr/lib/python2.7/plat-x86_64-linux-gnu\n",
      "/usr/lib/python2.7/lib-tk\n",
      "/usr/lib/python2.7/lib-old\n",
      "/usr/lib/python2.7/lib-dynload\n",
      "/home/jsl/.local/lib/python2.7/site-packages\n",
      "/usr/local/lib/python2.7/dist-packages\n",
      "/usr/lib/python2.7/dist-packages\n",
      "/usr/lib/python2.7/dist-packages/PILcompat\n",
      "/usr/lib/python2.7/dist-packages/gtk-2.0\n",
      "/usr/lib/python2.7/dist-packages/ubuntu-sso-client\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/extensions\n",
      "/home/jsl/.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* spark warehouse dir\n",
    "```\n",
    ".config('spark.sql.warehouse.dir', 'file:////home/jsl/Code/git/bb/jsl/pyds/data/')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 설정을 읽어 온다.\n",
    "    * spark-defaults.conf와 같은 파일의 값을 읽어서 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "117.16.44.45\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphframes:graphframes:0.4.0-spark2.0-s_2.11,org.mongodb.spark:mongo-spark-connector_2.10:2.0.0,com.databricks:spark-csv_2.11:1.5.0\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 RDD 소개\n",
    "\n",
    "* RDD (Resilient Distributed Dataset)는 레코드records의 집합으로, 여러 컴퓨터에 분산해서 사용할 수 있는 데이터 형식이다.\n",
    "    * Resilient - fault tolerent (어느 한 노드에서 작업이 실패하면 다른 노드에서 실행된다.)\n",
    "    * Distributed - multiple nodes in a clusters\n",
    "    * Dataset - 데이터타잎으로 구성된다.\n",
    "* RDD는 내, 외부 자료에서 생성하며, 생성된 자료는 read-only이다.\n",
    "    * HDFS 파일을 처리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 RDD 생성\n",
    "\n",
    "* 외부 파일 또는 배열과 같은 자료구조에서 읽어서 생성하고, 생성된 RDD는 분산하여 처리할 수 있게 된다.\n",
    "\n",
    "생성 방법 | 설명 | 함수\n",
    "----------|----------|----------\n",
    "외부에서 읽기 | 파일, HDFS, HBase 등 | textFile(\"mydir/\")<br>textFile(\"mydir/*.txt\")<br>textFile(\"mydir/*.gz\")<br>Hadoop InputFormat\n",
    "내부에서 읽기 | Pytho list에서 생성 | parallelize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list에서 RDD 생성하기\n",
    "* SparkSession을 통하거나 또는 직접 SparkContext를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)\n",
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 파일에서 RDD 생성하기\n",
    "    * Apache spark wiki에서 첫 문단을 복사해 왔다.\n",
    "    * 3째줄은 한글, 4째 줄은 같은 단어를 반복해 추가했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 비교\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "SparkSession.sparkContext.textFile() | 'SparkContext'를 사용하므로 RDD를 생성한다.\n",
    "SparkSession.read.text() | DataFrame을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(value=u'Wikipedia')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n"
     ]
    }
   ],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "print myRdd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* csv에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* RDD를 파일에서 읽어 생성하면, 전체가 하나의 record가 된다.\n",
    "* map()함수를 사용하면, 여러 record로 분리 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2', u'40, 27', u'12, 38', u'15, 31', u'21, 1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd3 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "myRdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4 = myRdd3.map(lambda line: line.split(','))\n",
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 RDD API\n",
    "\n",
    "* Transformations, Actions로 구분할 수 있다. Dataframe의 Transformer, Estimator와 비교할 수 있다.\n",
    "* 변환 transformations\n",
    "    * lazy연산을 한다. 즉, 실제 변환은 action이 수행되는 시점에 이루어진다.\n",
    "    * 결과는 RDD 또는 seq(RDD)\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "map(fn) | 요소별로 fn을 적용해서 결과 RDD 돌려줌 | .map(lambda x: x.split(' ')\n",
    "filter(fn) | 요소별로 선별하여 fn을 적용해서 결과 RDD 돌려줌 | .filter(lambda x: \"Spark\" in x)\n",
    "flatMap(fn) | 요소별로 fn을 적용하고, flat해서 결과 RDD 돌려줌 | .flatMap(lambda x: x.split(' '))\n",
    "groupByKey() | key를 그룹해서 iterator를 돌려줌. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* actions\n",
    "    * RDD를 값으로 변환한다. 예, Python list\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "reduce(fn) | 요소별로 fn을 사용해서 줄여서 결과 list를 돌려줌 |\n",
    "collect() | 모든 요소를 결과 list로 돌려줌 |\n",
    "count() | 요소의 갯수를 결과 list로 돌려줌 |\n",
    "countByKey() | key별 함수 |\n",
    "foreach(fn) | 각 데이터 항목에 함수fn을 적용 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  S.6.1 비슷한 Python 함수\n",
    "\n",
    "* map, reduce, filter\n",
    "    * 함수의 인자는 2개가 필요하다 (함수, 데이터).\n",
    "\n",
    "함수 | 설명 | 예\n",
    "-------|-------|-------\n",
    "map() | 각 데이터 요소에 함수를 적용해서 list를 반환 | map(fn,data)\n",
    "filter() | 각 데이터 요소에 함수의 결과 True를 선택해서 반환 | filter(fn, data)\n",
    "reduce() | 각 데이터 요소에 함수를 적용해서 list를 반환 | reduce(fn, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python 함수로 처리한다.\n",
    "    * 입출력은 데이터 하나씩이 아니라, list로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python에서 제공하는 map() 함수를 사용한다. map() 함수의 인자:\n",
    "    * (1) 함수명 (함수의 return은 반드시 있어야 한다.)\n",
    "    * (2) 입력인자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* lambda함수를 사용한다.\n",
    "    * lambda는 무명 함수이다.\n",
    "    * 처리 결과는 'return'을 사용하지 않아도 반환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 문자열에 map()을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hello World'\n",
    "words = sentence.split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 문자열을 사용하면, 각 단어를 split()한다.\n",
    "* list를 사용하면, 각 요소를 split()한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Hello World\"]\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* filter()는 데이터를 선별한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* reduce()는 2개의 인자를 받는다.\n",
    "* [ func(func(s1, s2),s3), ... , sn ]와 같이 수행한다.\n",
    "* x,y 인자는 두 수를 반복해서 더한다는 의미로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6.2 RDD 사용하기\n",
    "\n",
    "* transformation, action을 사용한다.\n",
    "* lambda함수를 또는 사용자함수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* map(), collect() 사용해서 square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x).collect()\n",
    "print squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* reduce()를 사용해서 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단순 통계 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 4 1.11803398875 1.25\n"
     ]
    }
   ],
   "source": [
    "print nRdd.sum(), nRdd.min(), nRdd.max(), nRdd.stdev(), nRdd.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* map()함수로 단어 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words=myRdd2.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* lambda아닌 사용자함수로 map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "words=myRdd2.map(mySplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in words.collect():\n",
    "    for word in line:\n",
    "        print word,\n",
    "    print \"\\n-----\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 각 문장의 철자 갯수를 센다.\n",
    "    * 첫 문장 'Wiskipedia'는 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print myRdd_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 한글은 앞에 u를 붙여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "print myRdd_unicode.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* filter()를 사용해서 stopwords 제거하기\n",
    "    * 문장 안에 stopwords를 포함한 경우는 제거되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " u\"Originally developed at the University of California, Berkeley's AMPLab,\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['is','am','are','the','for','a']\n",
    "myRdd_stop = myRdd2.filter(lambda x: x not in stopwords)\n",
    "myRdd_stop.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 문장 처리하기\n",
    "* 단어를 교체하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "a=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(a)\n",
    "\n",
    "words=_rdd.map(lambda x:x.split())\n",
    "print words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_upper=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "_upper.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 첫 글자를 대문자로 만들어서 출력해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: x[0].upper())\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: [i.upper() for i in x])\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* transformation(map()), action(collect()) 함수를 한꺼번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "pluralRDD=words.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print pluralRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = words\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print wordsLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 파일에 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pluralRDD.saveAsTextFile(\"data/ds_spark_wiki1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* groupBy\n",
    "    * 앞 2글자를 key로 사용해서, groupBy()\n",
    "    * 결과는 key-value, value는 iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c',\n",
       " u\"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " u'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " u'which has maintained it since.',\n",
       " u'Spark provides an interface for programming entire clusters with',\n",
       " u'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 앞 2글자로 grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "im implicit data parallelism and fault-tolerance.\n",
      "-----\n",
      "th the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n",
      "Wi Wikipedia\n",
      "-----\n",
      "Ap Apache Spark is an open source cluster computing framework.\n",
      "Ap Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "Sp Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "Or Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "wh which has maintained it since.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print k, eachValue\n",
    "    print \"-----\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* pair RDD\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "byKey | 동일한 키에 대해 연산<br>- 단계 1: key-value를 계산한다. 각 key의 빈도를 계산  '(key,1)'<br>- 단계 2: byKey를 적용한다. 동일한 key의 value를 더해준다.\n",
    "byValue | 예, mapValues\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "구분 | 설명\n",
    "-----|-----\n",
    "groupByKey | 같은 key를 grouping, 부분partition에서 먼저 reduce하지 않고, 전체로 계산한다.\n",
    "reduceByKey | 같은 key를 reduce, 부분partition에서 먼저 reduce하고, 전체로 계산한다. grouping + aggregation. 즉 reduceByKey = groupByKey().reduce()\n",
    "aggregateByKey() | reduceByKey()와 유사하지만 결과를 다른 형식으로 반환. For example (1,2),(1,4) as input and (1,\"six\") as output\n",
    "mapValues | value에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* reduceByKey - key별로 value를 합쳐서 결과 -> 아래는 a,3 b,2\n",
    "```\n",
    "(\"a\", 1)\n",
    "(\"b\", 1)\n",
    "(\"a\", 1)\n",
    "(\"a\", 1)\n",
    "(\"b\", 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터 예\n",
    "\n",
    "P1 | P2 | P3\n",
    "-----|-----|-----\n",
    "(key1,1)<br> | (key1,1)<br> | (key1,1)<br>\n",
    "(key1,1)<br> | (key2,1)<br> | (key1,1)<br>\n",
    "(key1,1)<br> |              | (key2,1)<br>\n",
    "(key2,1)<br> |              | (key2,1)<br>\n",
    "(key2,1)<br> |              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* reduceByKey: (key1,6) (key2,5)\n",
    "\n",
    "P1 | P2 | P3\n",
    "-----|-----|-----\n",
    "(key1,3)<br>(key2,2) | (key1,1)<br>(key2,1) | (key1,2)<br>(key2,2)\n",
    "\n",
    "* groupByKey: (key1,6) (key2,5)\n",
    "\n",
    "key1 | key2\n",
    "-----|-----\n",
    "(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1) | (key2,1)<br>(key2,1)<br>(key2,1)<br>(key2,1)<br>(key2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* flatMap()은 결과를 flatten해서, 단어의 갯수를 셀 수 있게 한다. 반면 map()은 그렇지 못하다.\n",
    "* groupByKey()는 key를 묶어준다.\n",
    "* 따라서 iterator를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', <pyspark.resultiterable.ResultIterable at 0x7fc74f5bd390>),\n",
       " (u'\\uc18c\\uc2a4', <pyspark.resultiterable.ResultIterable at 0x7fc74f5bd090>),\n",
       " (u'is', <pyspark.resultiterable.ResultIterable at 0x7fc74f5bd190>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* groupBy()를 한 후 mapValues(sum)을 하면 key별 합계를 구할 수 있다.\n",
    "* mapValues()는 기능을 value에 적용하는 함수\n",
    "    * 내장함수 sum()은 value의 합계를 내는 sum(value)을 의미\n",
    "    * 사용자 함수를 정의해서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 1),\n",
       " (u'\\uc18c\\uc2a4', 1),\n",
       " (u'is', 1),\n",
       " (u'Wikipedia', 1),\n",
       " (u'AMPLab,', 1),\n",
       " (u'maintained', 1),\n",
       " (u'donated', 1),\n",
       " (u'\\ucef4\\ud4e8\\ud305', 1),\n",
       " (u'open', 1),\n",
       " (u'since.', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'AMPLab,', 1),\n",
       " (u'Apache', 6),\n",
       " (u\"Berkeley's\", 1),\n",
       " (u'California,', 1),\n",
       " (u'Foundation,', 1),\n",
       " (u'Originally', 1),\n",
       " (u'Software', 1),\n",
       " (u'Spark', 7),\n",
       " (u'University', 1),\n",
       " (u'Wikipedia', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x): return len(x)\n",
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(f)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for문을 사용해서 출력을 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wc=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'AMPLab,', 1)\n",
      "(u'Apache', 6)\n",
      "(u\"Berkeley's\", 1)\n",
      "(u'California,', 1)\n",
      "(u'Foundation,', 1)\n",
      "(u'Originally', 1)\n",
      "(u'Software', 1)\n",
      "(u'Spark', 7)\n",
      "(u'University', 1)\n",
      "(u'Wikipedia', 1)\n"
     ]
    }
   ],
   "source": [
    "for e in wc:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: RDD를 사용하여 word count를 계산하고 그래프 그리기.\n",
    "\n",
    "* reduceByKey\n",
    "    * 같은 key에 대해 그 value를 reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc2=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "(7, u'Spark')\n",
      "(6, u'Apache')\n",
      "(5, u'\\uc544\\ud30c\\uce58')\n",
      "(4, u'\\uc2a4\\ud30c\\ud06c')\n",
      "(3, u'the')\n",
      "(2, u'an')\n",
      "(1, u'and')\n",
      "(1, u'\\uc18c\\uc2a4')\n",
      "(1, u'is')\n",
      "(1, u'Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "print type(wc2)\n",
    "for i in wc2:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* barh() horizontal bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAFdCAYAAAD2ROx1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSvv/OBPGdJHKtS4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a608dd9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = map(lambda x: x[0], wc2)\n",
    "word = map(lambda x: x[1], wc2)\n",
    "plt.barh(range(len(count)), count, color = 'grey')\n",
    "plt.yticks(range(len(count)), word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* combineByKey(x, y, z)\n",
    "\n",
    "구분 | 함수명 | 설명\n",
    "-----|-----|-----\n",
    "x | Combiner 함수 | 값을 combine (V) -> C 예: (value,1)\n",
    "y | Merge value 함수 | 값을 merge (C, V) -> C 예: (sum,count)\n",
    "z | Merge combiners 함수 | combiner를 merge (C, C) -> C) 예: (K, C)\n",
    "\n",
    "데이터 | 적용 함수 | 결과\n",
    "-----|-----|-----\n",
    "('kim',86) | combiner | kim, (86,1)\n",
    "('lim',87) | combiner | lim, (87,1)\n",
    "('kim',75) | merge value | (86,1)+(75,1) = (161,2)\n",
    "('kim',91) | merge value | (161,2)+(91,1) = (252,3)\n",
    "('lim',78) | merge value | (87,1)+(78,1) = (165,2)\n",
    "('lim',92) | merge value | (165,2)+(92,1) = (257,3)\n",
    "('lim',79) | merge value | (257,3)+(79,1) = (336,4)\n",
    "('lee',99) | combiner | (99,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lim', (336, 4)), ('lee', (99, 1)), ('kim', (252, 3))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marks = spark.sparkContext.parallelize([('kim',86),('lim',87),('kim',75),\n",
    "                                      ('kim',91),('lim',78),('lim',92),\n",
    "                                      ('lim',79),('lee',99)])\n",
    "marksByKey = marks.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "marksByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', (718.0, 4)), ('F', (326.0, 2))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights = spark.sparkContext.parallelize([\n",
    "        ('M',182.),('F',164.),('M',180.),('M',185.),('M',171.),('F',162.)\n",
    "    ])\n",
    "heightsByKey = heights.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "heightsByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 179.5, 'F': 163.0}\n"
     ]
    }
   ],
   "source": [
    "avgByKey = heightsByKey.map(lambda (label,(valSum,count)):\n",
    "                                (label,valSum/count))\n",
    "\n",
    "print avgByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 spark-submit\n",
    "\n",
    "* sys.path 설정은 하지 않아도 된다.\n",
    "\n",
    "* spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하였다.\n",
    "```\n",
    "log4j.rootCategory=ERROR, console\n",
    "```\n",
    "\n",
    "* Python 파일의 encoding 선언\n",
    "    * 기본 설정은 7-bit ASCII\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_hello.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 244ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/6ms)\n",
      "---------BEGIN-----------\n",
      "---------RESULT-----------\n",
      "2.0.0\n",
      "mean= 499.5\n",
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_rdd_hello.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_reduceBykey.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_reduceBykey.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "import os\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    myRdd=spark.sparkContext\\\n",
    "        .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "    res=myRdd\\\n",
    "        .flatMap(lambda x:x.split())\\\n",
    "        .map(lambda x:(x,1))\\\n",
    "        .reduceByKey(lambda x,y:x+y)\\\n",
    "        .map(lambda x:(x[1],x[0]))\\\n",
    "        .sortByKey(False)\\\n",
    "        .take(10)\n",
    "    for i in res:\n",
    "        print i\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 246ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/6ms)\n",
      "---------RESULT-----------\n",
      "(7, u'Spark')\n",
      "(6, u'Apache')\n",
      "(3, u'the')\n",
      "(2, u'an')\n",
      "(1, u'and')\n",
      "(1, u'\\uc18c\\uc2a4')\n",
      "(1, u'is')\n",
      "(1, u'Wikipedia')\n",
      "(1, u'AMPLab,')\n",
      "(1, u'maintained')\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_rdd_reduceBykey.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: RDD를 사용하여 word vector를 생성하기.\n",
    "\n",
    "* RDD API를 사용해서 단어를 셀 수 있다 (map, reduce 등).\n",
    "* mllib 패키지를 사용하여 데이터를 변환할 수 있다.\n",
    "    * TF-IDF, Word2Vec 등을 사용할 수 있다.\n",
    "    * mllib에 없는 변환기능은 ml을 사용한다 (ml은 DataFrame API 패키지.)\n",
    "        * Tokenizer, Stopwords, NGram 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ds_spark_wiki.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ds_spark_wiki.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 파일 전체 word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "lines=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "wc=lines\\\n",
    "    .flatMap(lambda x: x.split(' '))\n",
    "print type(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* RDD를 변환하면, 그 결과는 새로운 RDD로 반환된다.\n",
    "    * lazy연산이므로, collect()하는 시점에 실제 연산이 수행된다.\n",
    "* collect()의 결과는 list이다.\n",
    "    * 한글이 unicode로 출력되는 것은 깨진 것이 아니다. 하나씩 출력하면 한글로 출력된다.\n",
    "    * 모든 항목을 출력하려면 list의 인덱스 [:]를 사용한다.\n",
    "    * 10번째를 출력하려면 [10], 한글이 잘 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Wikipedia', u'Apache', u'Spark', u'is', u'an', u'open', u'source', u'cluster', u'computing', u'framework.', u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c\\ub294', u'\\uc624\\ud508', u'\\uc18c\\uc2a4', u'\\ud074\\ub7ec\\uc2a4\\ud130', u'\\ucef4\\ud4e8\\ud305', u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.', u'Apache', u'Spark', u'Apache', u'Spark', u'Apache', u'Spark', u'Apache', u'Spark', u'Originally', u'developed', u'at', u'the', u'University', u'of', u'California,', u\"Berkeley's\", u'AMPLab,', u'the', u'Spark', u'codebase', u'was', u'later', u'donated', u'to', u'the', u'Apache', u'Software', u'Foundation,', u'which', u'has', u'maintained', u'it', u'since.', u'Spark', u'provides', u'an', u'interface', u'for', u'programming', u'entire', u'clusters', u'with', u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']\n",
      "---> 한글로 출력: 아파치\n"
     ]
    }
   ],
   "source": [
    "print wc.collect()[:]\n",
    "print \"---> 한글로 출력:\",wc.collect()[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단어를 세어서 wc tuple로 만든다.\n",
    "    * 소문자로 만들고, 불필요한 구문 (new lines, commas, periods)을 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wc = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\\\n",
    "    .flatMap(lambda x: x.split(' '))\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아직 단어별로 갯수를 계산하지 않았기 때문에, 모두 1인 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'amplab', 1), (u'an', 1), (u'an', 1), (u'and', 1), (u'apache', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.sortByKey().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "print wc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단어의 갯수를 합계낸다.\n",
    "* 아래 모두 동일한 결과를 산출한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "reduceByKey(add) | 'add' operator를 사용하면, 단어튜플의 수를 키별로 더할 수 있다.\n",
    "groupByKey().mapValues(sum) | mapValues()를 사용하여 value의 'sum'을 계산할 수 있다.\n",
    "groupByKey().map(lambda (x,iter) : (x,len(iter))) | (key,value)의 구조를 사용하여 합계를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wcReduceByKey = wc.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcGroupByKey = wc.groupByKey().mapValues(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcGroupByKey2 = wc.groupByKey().map(lambda (x,v): (x,len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'amplab', 1),\n",
       " (u'an', 2),\n",
       " (u'and', 1),\n",
       " (u'apache', 6),\n",
       " (u'at', 1),\n",
       " (u\"berkeley's\", 1),\n",
       " (u'california', 1),\n",
       " (u'cluster', 1),\n",
       " (u'clusters', 1),\n",
       " (u'codebase', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcReduceByKey.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 라인 별 word count\n",
    "\n",
    "* dataframe으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'wikipedia', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'is', 1), (u'an', 1), (u'open', 1), (u'source', 1), (u'cluster', 1), (u'computing', 1), (u'framework', 1)]\n",
      "[(u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1), (u'\\uc624\\ud508', 1), (u'\\uc18c\\uc2a4', 1), (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1), (u'\\ucef4\\ud4e8\\ud305', 1), (u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1)]\n",
      "[(u'originally', 1), (u'developed', 1), (u'at', 1), (u'the', 1), (u'university', 1), (u'of', 1), (u'california', 1), (u\"berkeley's\", 1), (u'amplab', 1)]\n",
      "[(u'the', 1), (u'spark', 1), (u'codebase', 1), (u'was', 1), (u'later', 1), (u'donated', 1), (u'to', 1), (u'the', 1), (u'apache', 1), (u'software', 1), (u'foundation', 1)]\n",
      "[(u'which', 1), (u'has', 1), (u'maintained', 1), (u'it', 1), (u'since', 1)]\n",
      "[(u'spark', 1), (u'provides', 1), (u'an', 1), (u'interface', 1), (u'for', 1), (u'programming', 1), (u'entire', 1), (u'clusters', 1), (u'with', 1)]\n",
      "[(u'implicit', 1), (u'data', 1), (u'parallelism', 1), (u'and', 1), (u'fault', 1), (u'tolerance', 1)]\n"
     ]
    }
   ],
   "source": [
    "#from operator import add\n",
    "wc = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "    .map(lambda x:x.split())\\\n",
    "    .map(lambda x:[(i,1) for i in x])\n",
    "\n",
    "for e in wc.collect():\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* TF (Term Frequency)\n",
    "    * HashingTF: 단어ID로 Hash 값을 사용하여 단어빈도를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {253068: 1.0}),\n",
       " SparseVector(1048576, {36751: 1.0, 50570: 1.0, 68380: 1.0, 415281: 1.0, 511377: 1.0, 728364: 1.0, 862087: 1.0, 938426: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {63234: 1.0, 340190: 1.0, 357478: 1.0, 375592: 1.0, 458138: 1.0, 486171: 1.0, 598772: 1.0}),\n",
       " SparseVector(1048576, {938426: 4.0, 999480: 4.0}),\n",
       " SparseVector(1048576, {36757: 1.0, 225801: 1.0, 323305: 1.0, 453405: 1.0, 498679: 1.0, 518030: 1.0, 688842: 1.0, 762570: 1.0, 959994: 1.0}),\n",
       " SparseVector(1048576, {420843: 1.0, 550676: 1.0, 725041: 1.0, 782544: 1.0, 938426: 1.0, 959994: 2.0, 991590: 1.0, 993084: 1.0, 996703: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {50573: 1.0, 263739: 1.0, 892834: 1.0, 1014710: 1.0, 1035538: 1.0}),\n",
       " SparseVector(1048576, {3932: 1.0, 36751: 1.0, 192182: 1.0, 358969: 1.0, 363244: 1.0, 496856: 1.0, 546913: 1.0, 938426: 1.0, 951974: 1.0}),\n",
       " SparseVector(1048576, {69621: 1.0, 157580: 1.0, 219357: 1.0, 297436: 1.0, 715648: 1.0})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "tf.collect()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
