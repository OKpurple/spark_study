{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark 분석\n",
    "\n",
    "* Last updated 20170421 20161125\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark ETL을 할 수 있다.\n",
    "* Spark를 사용하여 구조적 데이터를 분석할 수 있다.\n",
    "* Spark를 사용하여 텍스트 분석을 할 수 있다.\n",
    "* Spark를 사용하여 추천을 할 수 있다.\n",
    "* Spark를 사용하여 그래프 분석을 할 수 있다.\n",
    "* 시각화\n",
    "    * matplotlib\n",
    "    * interactive Bokeh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.2 IPython Notebook에서 SparkSession 생성하기\n",
    "* S.3 데이터 타잎\n",
    "* S.3.1 vectors\n",
    "* S.3.2 labeled point\n",
    "* S.3.3 maxtrix\n",
    "* S.3.4 libsvm format\n",
    "* S.4 통계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* S.5 변환 \n",
    "* S.5.1 모델의 입력데이터로 변환\n",
    "* S.5.2 Python을 사용한 단어 빈도 계산\n",
    "* S.5.3 scikit-learn TF-IDF \n",
    "* S.5.4 StringIndexer\n",
    "* S.5.5 Tokenizer\n",
    "* S.5.6 RegTokenizer\n",
    "* S.5.7 Stopwords\n",
    "* S.5.8 CountVectorizer\n",
    "* S.5.9 TF-IDF\n",
    "* S.5.10 Word2Vec\n",
    "* S.5.11 NGram\n",
    "* S.5.12 연속데이터의 변환\n",
    "* 5.5.13 VectorAssembler\n",
    "* S.5.14 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* S.6 머신러닝\n",
    "* S.6.1 왜 머신러닝?\n",
    "* S.6.2 라이브러리\n",
    "* S.6.3 supervised vs unsupervised\n",
    "* S.6.4 회귀분석\n",
    "* S.6.5 군집화\n",
    "* S.6.6 분류\n",
    "* S.6.7 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* S.7 연속 데이터 분석\n",
    "* S.7.1 데이터\n",
    "* S.7.2 변환\n",
    "* S.7.3 KMeans\n",
    "* S.7.4 Regression\n",
    "* S.8 구조적 데이터 분석\n",
    "* S.8.1 데이터\n",
    "* S.8.2 변환\n",
    "* S.8.3 LogisticRegression\n",
    "* S.8.4 svm\n",
    "* S.8.5 Decision Tree\n",
    "* S.8.6 Naive Bayesian\n",
    "* S.9 텍스트 분석\n",
    "* S.9.1 데이터\n",
    "* S.9.2 변환\n",
    "* S.9.3 LogisticRegression\n",
    "* S.9.4 Decision Tree\n",
    "* S.9.5 Naive Bayesian\n",
    "* S.9.6 svm\n",
    "* S.9.7 LDA\n",
    "* S.10 추천\n",
    "* S.11 scikit-learn\n",
    "* S.12 그래프 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.3 문제 \n",
    "\n",
    "* 문제 S-1: 훈련데이터 만들기\n",
    "* 문제 S-2: Kolmogorov-Smirnov 검증\n",
    "* 문제 S-3: 평균, 표준편차와 같은 기본 통계 값을 구한다.\n",
    "* 문제 S-4: spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 IPython Notebook에서 SparkSession 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 데이터 타잎\n",
    "\n",
    "* Python은 데이터를 처리하면서 그 타잎을 엄격하게 따지지 않는다.\n",
    "* 반면에 Spark는 데이터를 효율적으로 처리하기 위해 데이터타잎을 사용한다.\n",
    "* Vector, Labeled Point, Matrix 모두 local vector이다.\n",
    "* Spark의 데이터 타잎:\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "Vector | numpy vector와 같은 특징을 가진다. dense와 sparse vector로 구분한다.\n",
    "Labeled Point | 클래스'label'와 속성'features'이 묶인 구조, supervised learning에 사용된다.\n",
    "Matrix | 수학의 행렬에 해당한다. numpy matrix에서와 같은 특징을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터 타잎은 Spark의 ml, mllib 패키지 별로 제공되므로, 구별하여 사용한다.\n",
    "\n",
    "패키지 | 설명\n",
    "-------|-------\n",
    "mllib | RDD API를 제공한다.\n",
    "ml | DataFrame API를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.1 vectors\n",
    "\n",
    "* Vector는 dense와 sparse로 구분할 수 있다.\n",
    "* sparse는 실제 값이 없는 요소, 즉 '0'을 제거하여 만든 vector이다.\n",
    "* Spark가 효율적으로 메모리를 사용하기 위해 자동으로 변환하여 사용하기도 한다.\n",
    "Spark에서 type field (1 바이트 길이)를 통해 식별한다 (0: sparse, 1: dense)\n",
    "\n",
    "\n",
    "dense vector | sparse vector\n",
    "----------|----------\n",
    "모든 행열 값을 가지고 있다. | 인덱스 및 값의 배열을 별도로 가진다.\n",
    "빈 값이 별로 없는 경우. | 빈 값이 많은 경우 사용. \n",
    "(160,69,24) | (3,[0,1,2],[160.0,69.0,24.0])<br>컬럼 3개, 값이 있는 컬럼, 값\n",
    "numpy array, Python list를 입력으로 사용 | Vectors.sparse(), SciPy’s csc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* dense vectors\n",
    "    * (1) Python list 또는 (2) numpy array를 dense vector로 사용할 수 있다.\n",
    "    * 내부적으로 numpy.array를 사용하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.1, 0.1] <type 'list'>\n"
     ]
    }
   ],
   "source": [
    "dv1=[0.0, 1.1, 0.1]\n",
    "print dv1, type(dv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* numpy로부터 dense vectors를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dv2 = np.array([1.0, 2.1, 3.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.1,0.1] <class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "dv1mllib=Vectors.dense(dv1)\n",
    "print dv1mllib, type(dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.1,0.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "dv1ml=Vectors.dense(dv1)\n",
    "print dv1ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* dense vectors는 numpy array와 같은 특징을 가진다.\n",
    "* 인덱스 방식으로 값을 읽을 수 있다. 또한 반복문에서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.1 0.1\n"
     ]
    }
   ],
   "source": [
    "for e in dv1ml:\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* product, dot, norm과 같은 벡터 연산을 할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.21,0.01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2200000000000002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print dv1ml*dv1ml\n",
    "dv1ml.dot(dv1ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* sparse vectors\n",
    "    * toArray()는 sparse에서 dense로 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.]\n"
     ]
    }
   ],
   "source": [
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "print sv1.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 참조 scipy.sparse의 Compressed Sparse Column 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2]\n",
      " [0 0 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "mtx = sps.csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "print mtx.todense()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.2 labeled point\n",
    "\n",
    "* 분류 및 회귀분석에 사용되는 데이터 타잎\n",
    "* 'label'과 'features'로 구성된다.\n",
    "    * 'label'은 supervised learning에서 '구분 값'으로 사용한다. 데이터타잎은 'Double'\n",
    "    * 'features'는 sparse, dense 모두 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[1.0,2.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "print LabeledPoint(1.0, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1992.0,(10,[0,1,2],[3.0,5.5,10.0]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "print LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 패키지를 혼용하는 경우 오류가 발생한다.\n",
    "    * mllib LabeledPoint와 ml Vectors를 혼용하면, 형변환 오류가 발생한다.\n",
    "    * 이러한 오류는 패키지를 혼용하지 않으면 된다.\n",
    "\n",
    "```\n",
    "Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [0.0,1.1,0.1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "LabeledPoint(1.0, dv1mllib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Vectors.fromML()를 사용해 ml의 Vectors를 읽어서 mllib로 변환하여 혼용을 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [0.0,1.1,0.1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "LabeledPoint(1.0, Vectors.fromML(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list에서 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1,[1.0,2.0,3.0]],[1,[1.1,2.1,3.1]],[0,[1.2,2.2,3.3]]]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list를 LabeledPoint로 생성하면, 'label'과 'features'의 명칭을 가지도록 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1,[1.0,2.0,3.0]),\n",
    "     LabeledPoint(1,[1.1,2.1,3.1]),\n",
    "     LabeledPoint(0,[1.2,2.2,3.3])]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* schema를 사용해서 DataFrame 생성하기\n",
    "    * 'label'은 DoubleType\n",
    "    * 'features'는 VectorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])\n",
    "\n",
    "trainDf=_rdd.toDF(schema)\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 사용자 함수udf User Defined Type를 사용하여 sparse vector를 dense vector로 변환해 보기\n",
    "    * 바로 변환할 수 있는 함수 toDense()를 지원하지 않는다.\n",
    "    * 따라서, sparse vector -> toArray() -> dense vector의 차례로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* sparse vector를 -> toArray()로 변환하는 함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.linalg.SparseVector'> [ 0.  1.  3.]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "print type(sv1),sv1.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 사용자 변환함수 udf()를 사용한다.\n",
    "* trainDf는 mllib RDD에서 변환된 데이터이므로, mllib 라이브러리를 사용한다.\n",
    "    * (1) sparse vector로 구성된 trainDf.features를 toArray()를 사용하여 array로 변환한다.\n",
    "        * VectorUDT()로 타잎을 지정한다. \n",
    "            * 지정하지 않으면 StringType을 기본으로, 자동으로 변환된다.\n",
    "    * (2) DenseVector()를 사용하여 array를 dense vector로 변환한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "from pyspark.mllib.linalg import DenseVector,VectorUDT | mllib\n",
    "from pyspark.ml.linalg import DenseVector,VectorUDT | ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.mllib.linalg import DenseVector, VectorUDT\n",
    "#myudf=udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "#myudf=udf(lambda x: Vectors.dense(x))\n",
    "myudf=udf(lambda x: DenseVector(x.toArray()), VectorUDT())\n",
    "_trainDf2=trainDf.withColumn('dvf',myudf(trainDf.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- dvf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|               dvf|\n",
      "+-----+--------------------+------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])| [0.0,1.0,0.0,5.5]|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|[-1.0,0.0,0.5,0.0]|\n",
      "+-----+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Rating\n",
    "    * 추천에서 사용한다.\n",
    "    * 사용자, 제품, 평가 항목으로 구성한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating(user=1, product=2, rating=5.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "Rating(1, 2, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.3 maxtrix\n",
    "\n",
    "* local matrix - pyspark.mllib.linalg.Matrix, Matrices\n",
    "* distributed matrix\n",
    "    * pyspark.mllib.linalg.distributed.RowMatrix\n",
    "    * pyspark.mllib.linalg.distributed.IndexedRow, IndexedRowMatrix\n",
    "    * pyspark.mllib.linalg.distributed.BlockMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: 훈련데이터 만들기\n",
    "\n",
    "* 공백으로 구분된 데이터를 식별하여 labeled point로 변환한다.\n",
    "* 읽을 대상이 파일이므로, RDD를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 읽기\n",
    "    * 파일이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#_fp=os.path.join(spark_home,\"data/mllib/sample_svm_data.txt\")\n",
    "_fp=os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "                 'data','mllib','sample_svm_data.txt')\n",
    "print os.path.isfile(_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_f=open(_fp,'r')\n",
    "_lines=_f.readlines()\n",
    "print _lines[0]\n",
    "_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 2.52078447201548,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.004684436494304,\n",
       " 2.000347299268466,\n",
       " 0.0,\n",
       " 2.228387042742021,\n",
       " 2.228387042742023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd=spark.sparkContext.textFile(_fp)\\\n",
    "    .map(lambda line: [float(x) for x in line.split(' ')])\n",
    "_rdd.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "_trainRdd0=_rdd.map(lambda line:LabeledPoint(line[0], line[1:]))\n",
    "_trainRdd0.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 공백을 분리하고, 분리된 데이터를 labeled point로 구성하는 기능을 합쳐서 실행해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd=spark.sparkContext.textFile(_fp)\\\n",
    "    .map(lambda line: [float(x) for x in line.split(' ')])\\\n",
    "    .map(lambda p:LabeledPoint(p[0], p[1:]))\n",
    "_trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다른 방식으로, 함수를 사용해서 변환해 본다.\n",
    "* 함수는 변환절차가 복잡할 경우 사용하면 편리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createLP(line):\n",
    "    p = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(p[0], p[1:])\n",
    "\n",
    "_rdd=spark.sparkContext.textFile(_fp)\n",
    "trainRdd = _rdd.map(createLP)\n",
    "\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.4 libsvm format\n",
    "\n",
    "* svm을 처리하기 위한 데이터 형식이다.\n",
    "* 0은 label, 나머지는 index:value 쌍으로 구성한다.\n",
    "```\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "```\n",
    "\n",
    "* 예\n",
    "```\n",
    "0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fsvm=os.path.join(os.environ[\"SPARK_HOME\"],'data','mllib','sample_libsvm_data.txt')\n",
    "dfsvm = spark.read.format(\"libsvm\").load(fsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsvm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsvm.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 통계\n",
    "\n",
    "* mllib 모듈을 사용한다 'pyspark.mllib.stat'\n",
    "* 기본 통계\n",
    "* 가설 검증\n",
    "* 상관관계 - 키와 몸무게의 상관관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2:  Kolmogorov-Smirnov 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.841344746068543 \n",
      "pValue = 5.06089025353873E-6 \n",
      "Very strong presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "parallelData = spark.sparkContext.parallelize([1.0, 2.0, 5.0, 4.0, 3.0, 3.3, 5.5])\n",
    "\n",
    "# run a KS test for the sample versus a standard normal distribution\n",
    "testResult = Statistics.kolmogorovSmirnovTest(parallelData, \"norm\", 0, 1)\n",
    "print(testResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3:  평균, 표준편차와 같은 기본 통계 값을 구한다.\n",
    "\n",
    "* 통계에 '무작위' 수는 중요하다. 무작위 샘플, 무작위 수를 발생하여 확률에서 빈번하게 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* DataFrame에서 제공하는 통계 기능을 사용해 본다.\n",
    "* 컬럼 3개의 DataFrame을 생성한다.\n",
    "    * 첫 컬럼은 'id', SparkSession.range()를 사용한다.\n",
    "    * 무작위 수를 추출해서, 나머지 컬럼 데이터를 만든다. pyspark.sql.functions 함수를 사용한다.\n",
    "    * rand()는 Uniform분포, randn()은 정규분포를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(0,10)\n",
    "df.show()\n",
    "df.select('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "| id|            uniform|             normal|\n",
      "+---+-------------------+-------------------+\n",
      "|  0|0.41371264720975787| 0.5888539012978773|\n",
      "|  1| 0.7311719281896606| 0.8645537008427937|\n",
      "|  2| 0.9031701155118229| 1.2524569684217643|\n",
      "|  3|0.09430205113458567| -2.573636861034734|\n",
      "|  4|0.38340505276222947| 0.5469737451926588|\n",
      "|  5| 0.5569246135523511|0.17431283601478723|\n",
      "|  6| 0.4977441406613893|-0.7040284633147095|\n",
      "|  7| 0.2076666106201438| 0.4637547571868822|\n",
      "|  8| 0.9571919406508957|  0.920722532496133|\n",
      "|  9| 0.7429395461204413|-1.4353459012380192|\n",
      "+---+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, randn\n",
    "colUniform = rand(seed=10).alias(\"uniform\")\n",
    "colNormal=randn(seed=27).alias(\"normal\")\n",
    "df3=df.select(\"id\", colUniform,colNormal)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 각 컬럼별로 통계 값을 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+--------------------+\n",
      "|summary|                id|            uniform|              normal|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "|  count|                10|                 10|                  10|\n",
      "|   mean|               4.5| 0.5488228646413278|0.009861721586543392|\n",
      "| stddev|3.0276503540974917| 0.2856822245344392|  1.2126061129356596|\n",
      "|    min|                 0|0.09430205113458567|  -2.573636861034734|\n",
      "|    max|                 9| 0.9571919406508957|  1.2524569684217643|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 무작위 수 2 컬럼을 만들어, 상관관계를 계산한다.\n",
    "* 자신과의 상관관계는 당연히 1.0이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.109939624671\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "print df.stat.corr('rand1', 'rand2')\n",
    "print df.stat.corr('id', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* freqItems()\n",
    "    * a, b, c 세 컬럼에 대해 60%이상 발생한 데이터를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([(1,2,3) if i%2==0 else (i,2*i,i%4) for i in range(100)],[\"a\",\"b\",\"c\"])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|a_freqItems|b_freqItems|c_freqItems|\n",
      "+-----------+-----------+-----------+\n",
      "|        [1]|        [2]|        [3]|\n",
      "+-----------+-----------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "freq = df.stat.freqItems([\"a\",\"b\",\"c\"],0.6)\n",
    "print freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 변환\n",
    "\n",
    "* Spark의 ETL (Etract, Transform, Load)은 다양한 입력에서 데이터를 추출, 변환하여 예측, 분류, 군집화, 추천과 같은 모델에 사용하게 된다.\n",
    "* RDD는 map-reduce와 같은 transform, action을 사용한다.\n",
    "* DataFrame은 Transformer, Estimator를 사용한다.\n",
    "* Pipeline은 여러 Estimator를 묶은 Estimator를 반환한다. 단계적으로 Estimator를 적용하기 위해 사용한다.\n",
    "\n",
    "변환 기능 | 설명 | 함수\n",
    "----------|----------|----------\n",
    "Estimator | 모델의 인자를 설정, 데이터에 적용한다. Transformer를 반환한다.| Estimator.fit()\n",
    "Transformer | 열을 선택, 변환한다. 그 결과를 DataFrame으로 반환한다. | Transformer.transform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.1 모델의 입력데이터로 변환\n",
    "\n",
    "* 군집화, 회귀분석, 분류, 추천 모델에 사용되는 데이터는 '일련의 수' 또는 '텍스트'로 구성된다.\n",
    "* 특징을 추출하여 feature vectors를 구성한다.\n",
    "* 분류를 하는 경우에는 class 또는 label 값이 필요하다.\n",
    "* 텍스트는 'bag of words'으로 표현한다.\n",
    "    * 문서는 단어로 구성된다.\n",
    "    * 단어의 순서는 의미를 가지지 않는다.\n",
    "    \n",
    "구분 | 설명 | 예\n",
    "----------|----------|----------|\n",
    "corpus | 문서 집합 | \"why she had to go\", \"where she have to go\"\n",
    "document | 레코드 | \"why she had to go\"\n",
    "vocabularay | 중복없는 단어 집합 | \"why\",\"she\",\"had\",\"to\",\"go\",\"where\",\"have\"\n",
    "word vector | 있다-없다, 단어빈도, TFIDF 사용할 수 있다.<br>dense, sparse 모두 가능하다. | [1,1,1,1,1,0,0],[0,1,0,1,1,1,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 데이터의 구성\n",
    "    * 데이터 컬럼 명은 'lableCol', 'featureCol'으로 설정해서 사용할 수 있다.\n",
    "    * 기본 컬럼 명을 사용하는 경우:\n",
    "        * DataFrame은 'label' (DoubleType), 'features' (sparse or dense vectors)\n",
    "        * Rdd는 LabeledPoint\n",
    "* 데이터 정련\n",
    "    * 결측 값, 범위를 벗어나는 outlier, \n",
    "    * trainRDD에 마이너스 값 nok?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.2 Python을 사용한 단어 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right 1\n",
      "be 7\n",
      "is 1\n",
      "When 1\n",
      "it 7\n",
      "in 3\n",
      "Mary 1\n",
      "Speaking 2\n",
      "standing 1\n",
      "darkness 1\n",
      "find 1\n",
      "wisdom, 3\n",
      "to 1\n",
      "Let 4\n",
      "And 1\n",
      "I 1\n",
      "let 3\n",
      "She 1\n",
      "words 3\n",
      "Mother 1\n",
      "front 1\n",
      "trouble 1\n",
      "me 2\n",
      "myself 1\n",
      "hour 1\n",
      "of 6\n",
      "times 1\n",
      "Whisper 1\n",
      "my 1\n",
      "comes 1\n"
     ]
    }
   ],
   "source": [
    "# Let it be 가사\n",
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]\n",
    "d={}\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1\n",
    "for k,v in d.iteritems():\n",
    "    print k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.3 scikit-learn TF-IDF\n",
    "\n",
    "* TfidfTransformer는 TF-IDF(Term Frequency-Inverse Document Frequency)를 계산한다.\n",
    "    * 단계 1: Tokenizer를 사용하여 문장을 단어로 분리 \n",
    "    * 단계 2: CountVectorizer를 사용하여 단어의 빈도수tf를 계산\n",
    "    * 단계 3: HashingTF를 사용하여 'word vector'를 계산.\n",
    "    HashingTF은 hash함수에 따라 단어의 고유번호를 생성,\n",
    "    hash고유번호의 충돌 가능성을 줄이기 위해, 단어 수를 제한할 수 있다.\n",
    "    * 단계 4: IDF를 계산\n",
    "    * 단계 4: TF-IDF를 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S.5.2.1 TF-IDF 계산\n",
    "\n",
    "* 'Let it be'가사 세 번째 줄 'wisdom'단어의 TF-IDF: 2.09861228867\n",
    "* TF-IDF를 'TfidfVectorizer'를 사용해서 곧 계산하면 그 결과를 볼 수 있다. \n",
    "\n",
    "```\n",
    "(2,12) 2.09861228867 <-- 2는 doc의 id (즉 3번째 문서), 12는 'wisdom'의 id (즉 단어의 id)\n",
    "```\n",
    "\n",
    "* 3번째 문장의 word vector를 구성해 본다.\n",
    "* id값은 모든 문장에서 단어를 추출하고 나서야 부여될 수 있다.\n",
    "\n",
    "words | id | 빈도 | \n",
    "-----|-----|-----\n",
    "Speaking | 7 | 1\n",
    "words | 13 | 1\n",
    "of | stopword | 0\n",
    "wisdom | 12 | 1\n",
    "let | 3 | 1\n",
    "it | stopword | 0\n",
    "be | stopword | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 위 word vector를 표로 나타내면 아래와 같다. 다른 행과 열은 이해를 돕기 위해 비워 놓았다.\n",
    "\n",
    "doc   | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |10 |11 |12 |13 |...\n",
    "------|---|---|---|---|---|---|---|---|---|---|---|---|---|---\n",
    "doc 0 |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "doc 1 |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "doc 2 | 1 |   |   |   |   |   | 1 |   |   |   |   | 1 | 1 |...\n",
    "...   |   |   |   |   |   |   |   |   |   |   |   |   |   |..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* TF-IDF 계산\n",
    "\n",
    "항목 | 설명 | 예제\n",
    "-----|-----|-----\n",
    "tf | term frequency 단어의 빈도 수 | $f_{t,d}$ / (number of words in d) = 1/4 = 0.25<br>(3번째 문서에 stopwords를 제외하면 4개의 단어, wisdom은 1회 나타난다.)\n",
    "df | document frequency 단어가 나타난 문서 수 | 3 (wisdom이 포함된 문서는 3)\n",
    "N | number of documents 전체 문서의 수 | 11 (전체의 문서는 11개)\n",
    "idf | inverse document frequency 단어가 나타난 문서의 비율을 거꾸로 | ln(N+1 / df+1) + 1 = log(12/4) + 1 = 1.09861 + 1<br>0으로 나뉘는 것을 방지하기 위해 smoothing, 즉 1을 더한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09861228867\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.2.2 sklearn을 사용한 TF-IDF\n",
    "\n",
    "* Spark는 'sklearn'의 TF-IDF와 동일한 방식으로 계산한다.\n",
    "* CountVectorizer를 사용하여, 문서 x 단어를 표로 계산결과를 출력할 수 있다.\n",
    "* 그 다음으로, TF-IDF를 계산할 수 있다. 이 때 (문서id, 단어id) 별로 결과가 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1]]\n",
      "{u'and': 0, u'be': 1, u'right': 17, u'whisper': 25, u'is': 8, u'it': 9, u'wisdom': 26, u'me': 12, u'let': 10, u'words': 27, u'in': 7, u'front': 5, u'trouble': 23, u'find': 4, u'standing': 20, u'comes': 2, u'myself': 15, u'darkness': 3, u'hour': 6, u'of': 16, u'when': 24, u'times': 21, u'to': 22, u'she': 18, u'mother': 13, u'my': 14, u'mary': 11, u'speaking': 19}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "vectorizer = CountVectorizer()\n",
    "print vectorizer.fit_transform(doc).todense()\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.79175946923\n",
      "  (0, 9)\t2.79175946923\n",
      "  (1, 0)\t2.79175946923\n",
      "  (1, 4)\t2.79175946923\n",
      "  (1, 5)\t2.79175946923\n",
      "  (2, 3)\t1.40546510811\n",
      "  (2, 12)\t2.09861228867\n",
      "  (2, 13)\t2.09861228867\n",
      "  (2, 7)\t2.38629436112\n",
      "  (3, 1)\t2.79175946923\n",
      "  (3, 2)\t2.79175946923\n",
      "  (4, 6)\t2.79175946923\n",
      "  (4, 8)\t2.79175946923\n",
      "  (5, 3)\t1.40546510811\n",
      "  (5, 12)\t2.09861228867\n",
      "  (5, 13)\t2.09861228867\n",
      "  (5, 7)\t2.38629436112\n",
      "  (6, 3)\t1.40546510811\n",
      "  (7, 3)\t1.40546510811\n",
      "  (8, 3)\t1.40546510811\n",
      "  (9, 3)\t1.40546510811\n",
      "  (10, 11)\t2.79175946923\n",
      "  (10, 3)\t1.40546510811\n",
      "  (10, 12)\t2.09861228867\n",
      "  (10, 13)\t2.09861228867\n",
      "{u'standing': 8, u'right': 6, u'darkness': 1, u'hour': 2, u'whisper': 11, u'times': 9, u'let': 3, u'speaking': 7, u'words': 13, u'mother': 5, u'trouble': 10, u'wisdom': 12, u'mary': 4, u'comes': 0}\n",
      "[ 2.79175947  2.79175947  2.79175947  1.40546511  2.79175947  2.79175947\n",
      "  2.79175947  2.38629436  2.79175947  2.79175947  2.79175947  2.79175947\n",
      "  2.09861229  2.09861229]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)\n",
    "\n",
    "print vectorizer.fit_transform(doc)\n",
    "print vectorizer.vocabulary_\n",
    "print vectorizer.idf_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* doc를 list of list로 만들어, DataFrame을 생성한다.\n",
    "* schema는 만들어 주지 않아도 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|sent                                  |\n",
      "+--------------------------------------+\n",
      "|When I find myself in times of trouble|\n",
      "|Mother Mary comes to me               |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|And in my hour of darkness            |\n",
      "|She is standing right in front of me  |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|우리 Let it be                          |\n",
      "|나 Let it be                           |\n",
      "|너 Let it be                           |\n",
      "|Let it be                             |\n",
      "|Whisper words of wisdom, let it be    |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]\n",
    "\n",
    "myDf=spark.createDataFrame(doc,['sent'])\n",
    "myDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.4 StringIndexer\n",
    "\n",
    "* 문자를 인덱스 값, double로 변환된다.\n",
    "\n",
    "구분 | 설명 | 예\n",
    "-----|-----|-----\n",
    "nominal | 명목 또는 구분 값 cateogry  | 사자, 호랑이, 사람\n",
    "ordinal | 명목값과 다른 점은 순서가 있다. | 키 low, med, high\n",
    "interval | 일정한 간격이 있다. | 150-165, 165-180, 180-195\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      9.0|\n",
      "|Mother Mary comes...|      8.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      5.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|        우리 Let it be|      6.0|\n",
      "|         나 Let it be|      1.0|\n",
      "|         너 Let it be|      2.0|\n",
      "|           Let it be|      7.0|\n",
      "|Whisper words of ...|      3.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")\n",
    "model=labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)\n",
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.5 Tokenizer\n",
    "\n",
    "* 문장을 단어와 같은 token으로 분리한다.\n",
    "* 단어는 배열로 구성한다. 요소는 string이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent=u'When I find myself in times of trouble', words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'])\n",
      "Row(sent=u'Mother Mary comes to me', words=[u'mother', u'mary', u'comes', u'to', u'me'])\n",
      "Row(sent=u'Speaking words of wisdom, let it be', words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(myDf)\n",
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.6 RegTokenizer\n",
    "\n",
    "* 단어를 분리하기 위한 패턴을 적용할 수 있다.\n",
    "* 한글에는 \\w 패턴이 적용되지 않는다. \n",
    "* 공백 \\s 패턴을 적용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|\n",
      "|         나 Let it be|    [나, let, it, be]|\n",
      "|         너 Let it be|    [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")\n",
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.7 Stopwords\n",
    "\n",
    "* 한 단어 등 불용어.\n",
    "* http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 현재 stop words에 자신의 것을 추가해서, 재설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_48ffb76d20f86d50eaea"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now d ll m o re ve y ain aren couldn didn doesn hadn hasn haven isn ma mightn mustn needn shan shouldn wasn weren won wouldn 나 너 우리\n"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 한글의 stop words '너','우리'가 제거되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|               [let]|\n",
      "|         나 Let it be|    [나, let, it, be]|               [let]|\n",
      "|         너 Let it be|    [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.8 CountVectorizer\n",
    "\n",
    "* 입력: a collection of text documents\n",
    "* 출력: word vector (sparse) vocabulary x TF\n",
    "* tokenize하고 나서 사용\n",
    "* minDF\n",
    "    * 소수점은 비율, 사용된 문서 수/전체 문서 수\n",
    "        * 정수는 사용된 문서 수, 단어가 몇 개의 문서에 사용되어야 하는지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[7,9],[1.0,1.0])|\n",
      "|She is standing r...|[standing, right,...|(16,[4,12,15],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|        우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,11],[1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "let wisdom, words speaking right trouble find hour times darkness mother whisper front mary comes standing\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\",\n",
    "    vocabSize=30,minDF=1.0)\n",
    "cvModel = cv.fit(stopDf)\n",
    "cvDf = cvModel.transform(stopDf)\n",
    "\n",
    "cvDf.collect()\n",
    "cvDf.select('sent','nostops','cv').show()\n",
    "for v in cvModel.vocabulary:\n",
    "    print v,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.9 TF-IDF\n",
    "\n",
    "* Term frequency-inverse document frequency (TF-IDF)\n",
    "* tokenizer하고 나서 사용해야 함. \n",
    "*  HashingTF  고정길이 word vectors.\n",
    "* IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=[u'find', u'times', u'trouble'], hash=SparseVector(50, {10: 1.0, 24: 1.0, 43: 1.0}))\n",
      "Row(nostops=[u'mother', u'mary', u'comes'], hash=SparseVector(50, {1: 1.0, 21: 1.0, 24: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'hour', u'darkness'], hash=SparseVector(50, {23: 1.0, 27: 1.0}))\n",
      "Row(nostops=[u'standing', u'right', u'front'], hash=SparseVector(50, {24: 1.0, 43: 1.0, 46: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=50)\n",
    "hashDf = hashTF.transform(stopDf)\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)\n",
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.10 Word2Vec\n",
    "\n",
    "* see wikipedia https://en.wikipedia.org/wiki/Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([-0.0367, 0.0097, 0.0479]))\n",
      "Row(w2v=DenseVector([-0.0482, 0.0223, 0.0095]))\n",
      "Row(w2v=DenseVector([0.052, -0.001, -0.0019]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=3,minCount=0,inputCol=\"words\",outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)\n",
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.11 NGram\n",
    "\n",
    "* unigram은 한 단어로, bigram은 두 단어로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|               words|              ngrams|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|[and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|[she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking words, ...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|         나 Let it be|    [나, let, it, be]|[나 let, let it, i...|\n",
      "|         너 Let it be|    [너, let, it, be]|[너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|     [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper words, w...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Row(words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'], ngrams=[u'when i', u'i find', u'find myself', u'myself in', u'in times', u'times of', u'of trouble'])\n",
      "Row(words=[u'mother', u'mary', u'comes', u'to', u'me'], ngrams=[u'mother mary', u'mary comes', u'comes to', u'to me'])\n",
      "Row(words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'], ngrams=[u'speaking words', u'words of', u'of wisdom,', u'wisdom, let', u'let it', u'it be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDf = ngram.transform(tokDf)\n",
    "ngramDf.show()\n",
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.12 연속데이터의 변환\n",
    "\n",
    "* 연속 데이터 - 몸무게(inches), 키(pounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")\n",
    "binDf = binarizer.transform(myDf)\n",
    "binDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)\n",
    "qdDf.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.5.13 VectorAssembler\n",
    "\n",
    "* 열을 묶어서 Vector열로 만든다.\n",
    "* string은 묶을 수 없다.\n",
    "* pyspark.ml.linalg.Vectors를 사용한다. (주의: pyspark.mllib.linalg.Vectors를 사용하지 않는다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+------+------+-------+-------+---------+\n",
      "| id|weight|height|weight2|height3| features|\n",
      "+---+------+------+-------+-------+---------+\n",
      "|1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "|2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "|3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "|4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "|5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "+---+------+------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")\n",
    "vaDf = va.transform(qdDf)\n",
    "vaDf.printSchema()\n",
    "vaDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.14 Pipeline\n",
    "\n",
    "* Pipeline은 여러 작업을 묶어, 순서대로 단계적으로 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0L, \"a b c d e spark\", 1.0),\n",
    "    (1L, \"b d\", 0.0),\n",
    "    (2L, \"spark f g h\", 1.0),\n",
    "    (3L, \"hadoop mapreduce\", 0.0),\n",
    "    (4L, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(df)\n",
    "myDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 머신러닝\n",
    "\n",
    "### S.6.1 왜 머신러닝?\n",
    "\n",
    "* 기계학습은 컴퓨터가 학습을 통해 문제를 해결하는 방식을 말한다.\n",
    "* 기계학습은 '사람'이 문제를 해결하는 방식을 흉내내려고 한다.\n",
    "* 기계학습은:\n",
    "    * 사람이 처리하기 어려운 대규모 데이터를 처리할 수 있다.\n",
    "    * 사람이 발견하지 못하는 패턴을 발견할 수 있다.\n",
    "    * 사람이 저지르기 쉬운 bias를 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* supervised vs unsupervised\n",
    "    * supervised learning은 분류 값이 있는 경우, 분류\n",
    "    * unsupervised learning은 분류 값이 없는 경우, clustering\n",
    "\n",
    "구분 | 비연속 | 연속 | 입력\n",
    "----------|----------|----------|----------\n",
    "Unspervised | KMeans, LDA | SVD, PCA | \n",
    "Supervised | Naive Bayesian, Decision Tree, | 회귀분석, Ensembles | Labeled Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.6.2 라이브러리\n",
    "\n",
    "* DataFrame API는 ml, RDD API는 mllib를 사용하고 있다.\n",
    "\n",
    "구분 | 설명 | 지원\n",
    "-------|-------|-------\n",
    "mllib | RDD API | 점차 지원을 줄여나감.\n",
    "ml | DataFrame API, Pipelines. | Spark 3.0부터 공식적으로 지원, 이것을 사용하는 것을 추천\n",
    "\n",
    "* 관련 라이브러리\n",
    "    * weka\n",
    "    * scikit-learn: Spark 기계학습 API가 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.6.3 회귀분석\n",
    "\n",
    "* 회귀분석은 분류 또는 예측에 사용.\n",
    "    * 입력 값은 1개 이상의 연속 값\n",
    "    * 결과 값은 연속 값\n",
    "* 회귀분석은 선형-비선형으로 구분.\n",
    "* 선형회귀모델은 OLS (ordinary least squares)를 사용하여 만든다. 즉 예측값과 실제값 차이의 제곱을 최소화한다.\n",
    "* shrinkage (regularization) - OLS weights를 줄이는 방식 (0까지 줄이면 속성 제거 효과)\n",
    "    * L2 ridge\n",
    "    * L1 lasso\n",
    "    * L2 + L1 (elastic net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.6.4 군집화\n",
    "\n",
    "* 군집의 갯수k를 정하고\n",
    "* 각 데이터 항목을 중심점과 얼마나 멀리 있는지 계산하여, 클러스터에 할당\n",
    "* 새로운 중심점을 갱신\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.6.5 classfication\n",
    "\n",
    "* 분류 모델\n",
    "\n",
    "구분 | ML \n",
    "-----|-----\n",
    "ml | LogisticRegression(trainDf)\n",
    "| DecisionTreeClassifier(labelCol=, featuresCol=)\n",
    "| LinearRegression\n",
    "| NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "mllib | SVMWithSGD.train(parsedData, iterations=100)\n",
    "| LogisticRegressionWithLBFGS.train(parsedData)\n",
    "| LinearRegressionWithSGD.train(parsedData)\n",
    "\n",
    "* 훈련과 테스트로 구분한다.\n",
    "\n",
    "```\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.6.6 추천\n",
    "\n",
    "* Collaborative Filtering\n",
    "* 취향이 유사한 사람들이 collaborative 좋아하는 제품을 선택filtering\n",
    "    * 홍길동이 좋아하는 영화가 '쾌도 홍길동'이고\n",
    "    * 장길산이 좋아하는 영화도 '쾌도 홍길동'이라면 \n",
    "    * 홍길동, 장길산은 같은 취향을 가지고 있다고 한다.\n",
    "    * 어떤 영화를 선정하면, 그 영화에 대해 유사한 선호를 가질 것이라고 추정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 연속데이터 분석\n",
    "\n",
    "* 군집과 회귀분석\n",
    "\n",
    "### S.7.1 데이터\n",
    "\n",
    "* tsv파일\n",
    "* 연속데이터\n",
    "* RDD에서 DataFrame을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.658985, 4.285136]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir=os.path.join(os.getenv('HOME'),'Code/git/else/machinelearninginaction/Ch10')\n",
    "filename=os.path.join(dir,'testSet.txt')\n",
    "\n",
    "rdd=spark.sparkContext.textFile(filename)\n",
    "myRdd=rdd.map(lambda line: [float(x) for x in line.split('\\t')])\n",
    "print myRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1=1.658985, _2=4.285136)]\n"
     ]
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(myRdd)\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.2 변환\n",
    "\n",
    "* VectorAssembler를 사용해서 'features' 컬럼을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"_1\",\"_2\"],outputCol=\"features\")\n",
    "trainDf = va.transform(myDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.3 KMeans\n",
    "\n",
    "* unsupervised clustering\n",
    "* mllib, ml 모두 KMeans 모델을 지원한다.\n",
    "    * k를 몇 개, 어떻게 정할지 설정한다.\n",
    "    * 중심점을 찾는 반복 회수를 설정한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "clusters = KMeans.train(myRdd, 2, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.2897198 , -2.83942545]), array([ 0.08249338,  2.94802785])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ml KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: double (nullable = true)\n",
      " |-- _2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(maxIter=10, initMode=\"random\", k=2)\n",
    "model = kmeans.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.2897198 , -2.83942545]), array([ 0.08249338,  2.94802785])]\n"
     ]
    }
   ],
   "source": [
    "print model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.7.4 Regressoin\n",
    "\n",
    "* 데이터는 위 키, 몸무게를 VectorAssembler로 묶은 vaDf를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol=\"weight2\", featuresCol=\"features\",\\\n",
    "                      maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(vaDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.8 구조적데이터 분석\n",
    "\n",
    "* 데이터 구조를 가지고 있는 경우\n",
    "* 머신러닝\n",
    "    * LogisticRegression\n",
    "    * svm \n",
    "    * Decision Tree\n",
    "    * Naive Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.8.1 데이터\n",
    "\n",
    "* 첫 컬럼은 label, 나머지는 속성\n",
    "* 모든 데이터 항목이 문자열이다.\n",
    "* nominal, ordinal 변수가 섞여 있다.\n",
    "    * 'Yes', 'No'는 순서가 없는 nominal 또는 category 명목변수이다.\n",
    "    * 'young', 'middle', 'old'는 순서가 있는 ordinal 형식이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'true', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'good'],\n",
    "        ['Yes','middle', 'true', 'true', 'good'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'excellent'],\n",
    "        ['No','old', 'false', 'false', 'fair'],\n",
    "    ],\n",
    "    ['cls','age','f1','f2','f3']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* DataFrame을 생성하면 'string'으로 인식한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.8.2 변환\n",
    "\n",
    "* 이 데이터에서 기계학습모델에서 사용하는 trainDf, trainRdd로 변환한다.\n",
    "* 기계학습모델에서 사용하는 형식을 따르도록 한다.\n",
    "    * Dataframe을 사용하는 'ml' 모듈은 'label', 'features'라는 명칭을 사용해야 한다.\n",
    "    * RDD를 사용하는 'mllib' 모듈은 LabeledPoint를 사용해야 한다.\n",
    "\n",
    "train 데이터 변수 명 | api 구분 | 패키지 | 데이터 형식\n",
    "-----|-----|-----|-----\n",
    "trainDf | DataFrame api | ml | 'label', 'features' (pyspark.ml.linalg.Vectors)\n",
    "trainRdd | RDD api | mllib | RDD LabeledPoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### trainDf 만들기\n",
    "\n",
    "transformer 구분 | 설명 | 예제\n",
    "----------|----------|----------\n",
    "StringIndexer | 문자열을 index로 변환한다.<br>기계학습에서 사용하는 'label' 명칭으로 변경. | 'cls'의 'Yes'를 0, 'No'를 1로 변환\n",
    "VectorAssembler | features 생성 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "clsIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"label\")\n",
    "i1Indexer = StringIndexer(inputCol=\"age\", outputCol=\"i1\")\n",
    "i2Indexer = StringIndexer(inputCol=\"f1\", outputCol=\"i2\")\n",
    "i3Indexer = StringIndexer(inputCol=\"f2\", outputCol=\"i3\")\n",
    "i4Indexer = StringIndexer(inputCol=\"f3\", outputCol=\"i4\")\n",
    "va = VectorAssembler(inputCols=[\"i1\",\"i2\",\"i3\",\"i4\"],outputCol=\"features\")\n",
    "#df3 = va.transform(df2)\n",
    "\n",
    "pipeline = Pipeline(stages=[clsIndexer,i1Indexer,i2Indexer,i3Indexer,i4Indexer,va])\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(df)\n",
    "df2 = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 모든 문자열 데이터를 index 값으로 변환하고, 묶어서 features로 변환하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- i1: double (nullable = true)\n",
      " |-- i2: double (nullable = true)\n",
      " |-- i3: double (nullable = true)\n",
      " |-- i4: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+------+-----+-----+---------+-----+---+---+---+---+-----------------+\n",
      "|cls|   age|   f1|   f2|       f3|label| i1| i2| i3| i4|         features|\n",
      "+---+------+-----+-----+---------+-----+---+---+---+---+-----------------+\n",
      "| No| young|false|false|     fair|  1.0|0.0|0.0|0.0|1.0|    (4,[3],[1.0])|\n",
      "| No| young|false|false|     good|  1.0|0.0|0.0|0.0|0.0|        (4,[],[])|\n",
      "|Yes| young| true|false|     good|  0.0|0.0|1.0|0.0|0.0|    (4,[1],[1.0])|\n",
      "|Yes| young| true| true|     fair|  0.0|0.0|1.0|1.0|1.0|[0.0,1.0,1.0,1.0]|\n",
      "| No| young|false|false|     fair|  1.0|0.0|0.0|0.0|1.0|    (4,[3],[1.0])|\n",
      "| No|middle|false|false|     fair|  1.0|1.0|0.0|0.0|1.0|[1.0,0.0,0.0,1.0]|\n",
      "| No|middle|false|false|     good|  1.0|1.0|0.0|0.0|0.0|    (4,[0],[1.0])|\n",
      "|Yes|middle| true| true|     good|  0.0|1.0|1.0|1.0|0.0|[1.0,1.0,1.0,0.0]|\n",
      "|Yes|middle|false| true|excellent|  0.0|1.0|0.0|1.0|2.0|[1.0,0.0,1.0,2.0]|\n",
      "|Yes|middle|false| true|excellent|  0.0|1.0|0.0|1.0|2.0|[1.0,0.0,1.0,2.0]|\n",
      "|Yes|   old|false| true|excellent|  0.0|2.0|0.0|1.0|2.0|[2.0,0.0,1.0,2.0]|\n",
      "|Yes|   old|false| true|     good|  0.0|2.0|0.0|1.0|0.0|[2.0,0.0,1.0,0.0]|\n",
      "|Yes|   old| true|false|     good|  0.0|2.0|1.0|0.0|0.0|[2.0,1.0,0.0,0.0]|\n",
      "|Yes|   old| true|false|excellent|  0.0|2.0|1.0|0.0|2.0|[2.0,1.0,0.0,2.0]|\n",
      "| No|   old|false|false|     fair|  1.0|2.0|0.0|0.0|1.0|[2.0,0.0,0.0,1.0]|\n",
      "+---+------+-----+-----+---------+-----+---+---+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* trainDf를 완성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDf=df2.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|        (4,[],[])|\n",
      "|  0.0|    (4,[1],[1.0])|\n",
      "|  0.0|[0.0,1.0,1.0,1.0]|\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|[1.0,0.0,0.0,1.0]|\n",
      "|  1.0|    (4,[0],[1.0])|\n",
      "|  0.0|[1.0,1.0,1.0,0.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,2.0]|\n",
      "|  1.0|[2.0,0.0,0.0,1.0]|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### trainRdd 만들기\n",
    "\n",
    "* RDD trainRdd를 만든다.\n",
    "* LabeledPoint는 df에서 컬럼을 선택해서 만든다.\n",
    "    * LabeledPoint는 RDD이다 (pyspark.mllib.regression.LabeledPoint)\n",
    "* 주의: ml vs mllib의 혼용\n",
    "    * 'row.features'는 pyspark.ml.feature import VectorAssembler로 생성된 클래스\n",
    "    * 그대로 사용하면 오류가 발생한다.\n",
    "\n",
    "```\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "trainRdd = trainDf.rdd.map(lambda row: LabeledPoint(row.label,row.features))\n",
    "trainRdd.first() -> 오류 'Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector'\n",
    "```\n",
    "\n",
    "    * pyspark.mllib.linalg.Vectors.fromML()을 사용해서 혼용을 막아 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainDf.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Dataframe(trainDf)을 rdd로 변환한다.\n",
    "    * DataFrame을 RDD로 변환한후 (1) map()함수를 사용하여 LabeledPoint로 변환하거나, (2) 직접 Row()로 변환\n",
    "    * features의 구성이 Dense 또는 Sparse로 섞여 있다.\n",
    "* 주의: map()은 lazy evaluation이므로 (사용할 때 실제로 변환이 이루어짐), 사용할 때 Vectors 클래스가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, (4,[3],[1.0])),\n",
       " LabeledPoint(1.0, (4,[],[])),\n",
       " LabeledPoint(0.0, (4,[1],[1.0])),\n",
       " LabeledPoint(0.0, [0.0,1.0,1.0,1.0]),\n",
       " LabeledPoint(1.0, (4,[3],[1.0])),\n",
       " LabeledPoint(1.0, [1.0,0.0,0.0,1.0]),\n",
       " LabeledPoint(1.0, (4,[0],[1.0])),\n",
       " LabeledPoint(0.0, [1.0,1.0,1.0,0.0]),\n",
       " LabeledPoint(0.0, [1.0,0.0,1.0,2.0]),\n",
       " LabeledPoint(0.0, [1.0,0.0,1.0,2.0]),\n",
       " LabeledPoint(0.0, [2.0,0.0,1.0,2.0]),\n",
       " LabeledPoint(0.0, [2.0,0.0,1.0,0.0]),\n",
       " LabeledPoint(0.0, [2.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [2.0,1.0,0.0,2.0]),\n",
       " LabeledPoint(1.0, [2.0,0.0,0.0,1.0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.mllib.util import Vectors\n",
    "\n",
    "trainRdd = trainDf.rdd.map(lambda row: LabeledPoint(row.label,Vectors.fromML(row.features)))\n",
    "#trainRdd = trainDf.rdd.map(lambda row: LabeledPoint(row.label,Vectors(row.features)))\n",
    "trainRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([2.0, 0.0, 0.0, 1.0])),\n",
       " Row(label=1.0, features=SparseVector(4, {0: 2.0})),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 0.0, 0.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([2.0, 0.0, 0.0, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([1.0, 0.0, 0.0, 1.0])),\n",
       " Row(label=1.0, features=SparseVector(4, {0: 1.0})),\n",
       " Row(label=0.0, features=DenseVector([1.0, 1.0, 1.0, 0.0])),\n",
       " Row(label=0.0, features=DenseVector([1.0, 0.0, 1.0, 2.0])),\n",
       " Row(label=0.0, features=DenseVector([1.0, 0.0, 1.0, 2.0])),\n",
       " Row(label=0.0, features=DenseVector([0.0, 0.0, 1.0, 2.0])),\n",
       " Row(label=0.0, features=SparseVector(4, {2: 1.0})),\n",
       " Row(label=0.0, features=SparseVector(4, {1: 1.0})),\n",
       " Row(label=0.0, features=DenseVector([0.0, 1.0, 0.0, 2.0])),\n",
       " Row(label=1.0, features=SparseVector(4, {3: 1.0}))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd=trainDf.rdd\n",
    "print type(_trainRdd)\n",
    "_trainRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.8.3 LogisticRegression\n",
    "\n",
    "* supervised learning\n",
    "* 이메일이 spam인지 아닌지 또는 거래가 정상인지 아닌지의 이진분류하는 문제\n",
    "* logistic(sigmoid)함수를 사용해서 확률을 강제적으로 0 또는 1로 이진분류한다.\n",
    "    * $log\\frac{p}{1-p} \\in \\{1, 0\\}$\n",
    "        * 0\n",
    "        * 1\n",
    "* Spark는 이진분류만 가능하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### pyspark.ml.classification.LogisticRegression\n",
    "\n",
    "* 모델을 설정하여 생성한다. 이진분류만 가능하다.\n",
    "* 학습된 모델로부터 속성의 계수를 출력할 수 있다.\n",
    "* 이 계수 값을 가지고, 예측에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "lrModel = lr.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.50705810019,-5.31916107407,-5.04694958332,-0.351455356638]\n",
      "3.2822908185\n"
     ]
    }
   ],
   "source": [
    "print lrModel.coefficients\n",
    "print lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9167192614845545"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*lrModel.coefficients[0]+1*lrModel.coefficients[3]+lrModel.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 주의\n",
    "    * parallelize()를 사용할 경우, pyspark.ml.linalg.Vectors,VectorUDT를 사용한다. mllib는 오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "#from pyspark.mllib.linalg import Vectors,VectorUDT\n",
    "from pyspark.sql import Row\n",
    "test0 = spark.sparkContext.parallelize([Row(features=Vectors.dense(2,0,0,1))]).toDF()\n",
    "result = model1.transform(test0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### pyspark.mllib.classification.LogisticRegression\n",
    "\n",
    "* mllib는 최적화 방식에 따라 모델을 지원한다.\n",
    "    * Stochastic gradient descent (SGD)\n",
    "    * LBFGS Limited-memory BFGS\n",
    "* mllib는 다항분류가 가능하다.\n",
    "* 주의: map()은 lazy evaluation이고, 사용되는 시점에시 실제 변환이 발생한다. 따라서 Vectors 클래스가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "lrm = LogisticRegressionWithLBFGS.train(trainRdd, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 예측\n",
    "    * 모델의 predict() 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict([1.0,0.0,1.1,1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelsAndPreds=trainRdd.map(lambda p: (p.label, lrm.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1),\n",
       " (1.0, 0),\n",
       " (0.0, 0),\n",
       " (0.0, 0),\n",
       " (1.0, 1),\n",
       " (1.0, 1),\n",
       " (1.0, 1),\n",
       " (0.0, 0),\n",
       " (0.0, 0),\n",
       " (0.0, 0),\n",
       " (0.0, 0),\n",
       " (0.0, 0),\n",
       " (0.0, 0),\n",
       " (0.0, 0),\n",
       " (1.0, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsAndPreds.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 오류\n",
    "    * 예측결과가 맞지 않는 비율을 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.0666666666667\n"
     ]
    }
   ],
   "source": [
    "trainErr = labelsAndPreds\\\n",
    "    .filter(lambda(v,p): v != p).count() / float(trainRdd.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델을 저장할 수 있다.\n",
    "    * 디렉토리를 지정하면 메타데이터를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(sc, \"myModelPath\")\n",
    "sameModel = LogisticRegressionModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.8.4 svm\n",
    "\n",
    "* **$L_d$**는 변수가 간단해짐. $\\alpha$를 풀고난 후, 이미 알고 있는 데이터 x,y를 대입해서 해를 구함.\n",
    "\n",
    "* Dual form (Wolfe dual)\n",
    "    * 라그랑주승수로 푼 편미분 결과를 $L_p$에 대입해서 풀면 -> w를 제거하게 됨.\n",
    "  $$\n",
    "    \\begin{aligned}\n",
    "    L_p &= \\frac{1}{2}\\|w\\|^2 - \\sum_{i=1}^{n}{\\alpha_i[y_i(w \\cdot x_i + b)-1]}\\\\\n",
    "    \\text{substitute w = $\\sum \\alpha y x$}\\\\\n",
    "        &= \\frac{1}{2}(\\sum \\alpha yx)^2 - \\sum {\\alpha [y (\\sum \\alpha y x x + b)-1]}\\\\\n",
    "        &= \\frac{1}{2}(\\sum \\alpha yx)^2 - \\sum \\sum \\alpha\\alpha y y x x + b \\sum \\alpha y + \\sum \\alpha\\\\\n",
    "    \\text{substitute $\\sum \\alpha y = 0$}\\\\\n",
    "    L_d&=\\sum_{i=1}^n \\alpha_i\n",
    "        -\\frac12 \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j K(x_i, x_j) \\alpha_i \\alpha_j\\\\\n",
    "    \\text{subject to:}\\\\\n",
    "            \\sum_{i=1}^n y_i \\alpha_i &= 0\\\\\n",
    "            \\alpha_i                  &\\ge 0\\\\\n",
    "            for\\ i=1, 2, \\cdots, n\n",
    "            \\end{aligned}\n",
    "        $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### mllib svm\n",
    "\n",
    "* svm은 입력데이터를 분류\n",
    "* ml은 svm을 지원하고 있지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "svm = SVMWithSGD.train(trainRdd, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.predict([1.0,0.0,1.1,1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.8.5 Decision Tree\n",
    "\n",
    "* Gini impurity entropy로 불확실성을 계산한다. 즉 50-50은 가장 불확실하다, \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(S) &= - \\sum_{i=1}^{n}\\ p_i\\ log_2\\ p_i\\\\\n",
    "     &= - p_1 log_2\\ p_1 - p_2 log_2\\ p_2\\ \\ldots\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "* 정보이득을 계산해서, 가장 높은 속성을 선정하여 분기\n",
    "\n",
    "$$\n",
    "IG(A,S) = H(S) - \\sum_{i=1}^m\\ p(i)\\ H(i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ml Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------------+\n",
      "|prediction|_label|        _features|\n",
      "+----------+------+-----------------+\n",
      "|       1.0|   1.0|[2.0,0.0,0.0,1.0]|\n",
      "|       1.0|   1.0|    (4,[0],[2.0])|\n",
      "|       0.0|   0.0|[2.0,1.0,0.0,0.0]|\n",
      "|       0.0|   0.0|[2.0,1.0,1.0,1.0]|\n",
      "|       1.0|   1.0|[2.0,0.0,0.0,1.0]|\n",
      "|       1.0|   1.0|[1.0,0.0,0.0,1.0]|\n",
      "|       1.0|   1.0|    (4,[0],[1.0])|\n",
      "|       0.0|   0.0|[1.0,1.0,1.0,0.0]|\n",
      "|       0.0|   0.0|[1.0,0.0,1.0,2.0]|\n",
      "|       0.0|   0.0|[1.0,0.0,1.0,2.0]|\n",
      "|       0.0|   0.0|[0.0,0.0,1.0,2.0]|\n",
      "|       0.0|   0.0|    (4,[2],[1.0])|\n",
      "|       0.0|   0.0|    (4,[1],[1.0])|\n",
      "|       0.0|   0.0|[0.0,1.0,0.0,2.0]|\n",
      "|       1.0|   1.0|    (4,[3],[1.0])|\n",
      "+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "li1 = StringIndexer(inputCol=\"cls\", outputCol=\"_label\")\n",
    "li2 = StringIndexer(inputCol=\"age\", outputCol=\"att1\")\n",
    "li3 = StringIndexer(inputCol=\"f1\", outputCol=\"att2\")\n",
    "li4 = StringIndexer(inputCol=\"f2\", outputCol=\"att3\")\n",
    "li5 = StringIndexer(inputCol=\"f3\", outputCol=\"att4\")\n",
    "va = VectorAssembler(inputCols=[\"att1\",\"att2\",\"att3\",\"att4\"],outputCol=\"_features\")\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"_label\", featuresCol=\"_features\")\n",
    "pipeline = Pipeline(stages=[li1,li2,li3,li4,li5,va,dt])\n",
    "model = pipeline.fit(df)\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"_label\", \"_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.8.6 Naive Bayesian\n",
    "\n",
    "* for c $\\in$ C 모든 클래스에 대해\n",
    "    * $ \\frac{N_c} {N}$ 클래스가 발생할 사전확률을 구한다 ($N_c$ 클래스 발생갯수, N: 전체갯수)\n",
    "    * 가능도 likelihood 갱신\n",
    "    * ML (Maximum likelihood estimation)를 계산하여 결정한다.\n",
    "        * $Y_{ML}=argmax_Y P(X | Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ml - bayesian\n",
    "\n",
    "* labeled point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.530628251062,-0.887303195001]\n",
      "DenseMatrix([[-1.35454566, -1.64222774, -1.48807706, -1.13140211],\n",
      "             [-0.57536414, -2.77258872, -2.77258872, -1.16315081]])\n"
     ]
    }
   ],
   "source": [
    "print model.pi\n",
    "print model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=model.transform(test0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ml의 Vectors, VectorUDT를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors,VectorUDT\n",
    "#from pyspark.mllib.linalg import Vectors,VectorUDT\n",
    "from pyspark.sql import Row\n",
    "test0 = spark.sparkContext.parallelize([Row(features=Vectors.dense([1.0,0.0,1.1,1.2]))]).toDF()\n",
    "result = model.transform(test0).head()\n",
    "result.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### mllib - Bayesian\n",
    "\n",
    "* spark 제공 자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parseLine(line):\n",
    "    parts = line.split(',')\n",
    "    label = float(parts[0])\n",
    "    features = Vectors.dense([float(x) for x in parts[1].split(' ')])\n",
    "    print features\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "data = spark.sparkContext.textFile('/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/data/mllib/sample_naive_bayes_data.txt').map(parseLine)\n",
    "\n",
    "# Split data aproximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NaiveBayes.train(training, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.9 텍스트 분석\n",
    "\n",
    "* 텍스트는 word vector를 생성해서 분석한다.\n",
    "* 머신러닝\n",
    "    * LogisticRegression\n",
    "    * Decision Tree\n",
    "    * Naive Bayesian\n",
    "    * LDA\n",
    "* 정서분석 http://text-processing.com/demo/sentiment/\n",
    "* kaggle\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.9.1 데이터\n",
    "\n",
    "* 입력은 string으로 한다.\n",
    "    * Array 입력은 맞지 않는다.\n",
    "```\n",
    "[0,['my','dog','has','flea','problems','help','please']]\n",
    "또는\n",
    "[0,['my dog has flea problems. help please.']]\n",
    "```\n",
    "    * 한글은 unicode로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        [0,'my dog has flea problems. help please.'],\n",
    "        [1,'maybe not take him to dog park stupid'],\n",
    "        [0,'my dalmation is so cute. I love him'],\n",
    "        [1,'stop posting stupid worthless garbage'],\n",
    "        [0,'mr licks ate my steak how to stop him'],\n",
    "        [1,'quit buying worthless dog food stupid'],\n",
    "        [0,u'우리 강아지 벌레 있어요 도와주세요'],\n",
    "        [0,u'우리 강아지 귀여워 너 사랑해'],\n",
    "        [1,u'강아지 공원 가지마 바보같이'],\n",
    "        [1,u'강아지 음식 구매 마세요 바보같이']\n",
    "    ],\n",
    "    ['cls','sent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.9.2 변환\n",
    "\n",
    "transformers, etsimators | 설명\n",
    "----------|----------\n",
    "StringIndexer | 데이터 -> index\n",
    "RegexTokenizer | 텍스트 -> 단어\n",
    "Stopwords | 단어에서 Stopwords 제거\n",
    "HashingTF | 단어 -> word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, HashingTF, StopWordsRemover, RegexTokenizer\n",
    "stopwords=list()\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"label\")\n",
    "regexTok = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsRegex\", pattern=\"\\\\s+\")\n",
    "#tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "stop = StopWordsRemover(inputCol=\"wordsRegex\", outputCol=\"nostops\")\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"nostops\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[labelIndexer,regexTok,stop,hashingTF])\n",
    "model=pipeline.fit(df)\n",
    "trainDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "|cls|label|            features|\n",
      "+---+-----+--------------------+\n",
      "|  0|  0.0|(262144,[25688,38...|\n",
      "|  1|  1.0|(262144,[55639,71...|\n",
      "|  0|  0.0|(262144,[46165,81...|\n",
      "|  1|  1.0|(262144,[57368,87...|\n",
      "|  0|  0.0|(262144,[69384,88...|\n",
      "|  1|  1.0|(262144,[57368,75...|\n",
      "|  0|  0.0|(262144,[14050,28...|\n",
      "|  0|  0.0|(262144,[28575,57...|\n",
      "|  1|  1.0|(262144,[28575,17...|\n",
      "|  1|  1.0|(262144,[28575,84...|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.select('cls','label','features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (262144,[25688,38977,75919,87576,239859],[1.0,1.0,1.0,1.0,1.0]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainRdd = trainDf\\\n",
    "    .rdd\\\n",
    "    .map(lambda row: LabeledPoint(row.label,Vectors.fromML(row.features)))\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.9.3 LogisticRegression\n",
    "\n",
    "* 분류에 사용하고, 2진 (ml) 또는 다분류 (mllib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "lrModel = lr.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* summary:  <pyspark.ml.classification.BinaryLogisticRegressionTrainingSummary object at 0x7f4252a7fe50>\n",
      "* coefficients:  (262144,[14050,25688,28575,38977,46165,55639,57298,57368,69384,71826,75919,81486,84127,87345,87576,88312,103148,104257,121133,121786,158088,163197,173036,175143,186022,186480,202268,220548,228185,239859,243184,244249,245237,250806,251329],[-1.27222161806,-1.0959340667,-0.0755352913925,-1.0959340667,-1.29204386194,1.13289974307,-1.75263828924,1.16070634922,-1.0381881797,1.13289974307,0.376841766526,-1.29204386194,1.19058324228,1.22114617531,-1.0959340667,-1.0381881797,0.842331778855,-1.27222161806,0.842331778855,1.19058324228,-1.75263828924,1.22114617531,1.6328492213,1.13289974307,0.842331778855,-1.29204386194,0.102913872536,-1.0381881797,1.58818076076,-1.0959340667,-1.27222161806,-1.0381881797,1.19058324228,1.6328492213,1.36987615596])\n",
      "* intercept:  -0.596615892431\n"
     ]
    }
   ],
   "source": [
    "print \"* summary: \", lrModel.summary\n",
    "print \"* coefficients: \", lrModel.coefficients\n",
    "print \"* intercept: \", lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "lrm = LogisticRegressionWithSGD.train(trainRdd, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.9.4 Decision Tree\n",
    "\n",
    "* ml.decisiontree\n",
    "    * cvDf의 label은 IllegalArgumentException, StringIndexer로 변환해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "model=dt.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.numNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, [1.0,1.0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "#_rdd=df.rdd.map(lambda x:LabeledPoint(x.class,[1,1]))\n",
    "_rdd=df.rdd.map(lambda x:LabeledPoint(x.cls,[1,1]))\n",
    "_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.9.5 Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* mllib\n",
    "    * 위에 있는 trainDf를 사용해서\n",
    "    * features가 ml Vectors이므로 변환해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.classification.NaiveBayesModel at 0x7f4252b5c290>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "NaiveBayes.train(trainRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.9.6 svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "svm = SVMWithSGD.train(trainRdd, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.9.7 LDA\n",
    "\n",
    "* Latent Dirichlet allocation (LDA)\n",
    "* 토픽모델링\n",
    "* Gibbs sampling\n",
    "* 다항분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*  LDA는 word vector를 입력으로 사용해서 (nword_doc, 문서별 단어의 수), 토픽에 단어를 배정한다 (nword_topic)\n",
    "\n",
    "변수명 | 설명\n",
    "-----|-----\n",
    "nword 또는 'V' | 단어의 수, len(CountVectorizer.vocabulary)\n",
    "ntopic 또는 'K' | 토픽의 수, 임의로 정한다 (예: 10개)\n",
    "ndoc 또는 'M' | 문서의 수, DataFrame.count()\n",
    "nword_doc | 'nword' x 'ndoc' 문서별 단어의 수\n",
    "nword_topic | 'nword' x 'ntopic' 주제별 단어의 수 \n",
    "ndoc_topic | 'ndoc' x 'ntopic' 주제별 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA\n",
    "#from pyspark.mllib.linalg import SparseVector, Vector, Vectors\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer\n",
    "\n",
    "regexTok = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsRegex\", pattern=\"\\\\s+\")\n",
    "reDf=regexTok.transform(df)\n",
    "cv = CountVectorizer(inputCol=\"wordsRegex\", outputCol=\"cv\")\n",
    "cvModel=cv.fit(reDf)\n",
    "cvDf=cvModel.transform(reDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- wordsRegex: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cv: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, Vector, Vectors\n",
    "testRdd = cvDf.select(\"cls\", \"cv\").rdd.map(lambda (x,y): [x,Vectors.fromML(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어수:  46\n",
      "문서수:  10\n",
      "0 강아지 -\n",
      "1 stupid -\n",
      "2 him -\n",
      "3 my -\n",
      "4 dog -\n",
      "5 to -\n",
      "6 stop -\n",
      "7 worthless -\n",
      "8 우리 -\n",
      "9 바보같이 -\n",
      "10 not -\n",
      "11 공원 -\n",
      "12 cute. -\n",
      "13 take -\n",
      "14 dalmation -\n",
      "15 garbage -\n",
      "16 quit -\n",
      "17 mr -\n",
      "18 park -\n",
      "19 is -\n",
      "20 사랑해 -\n",
      "21 problems. -\n",
      "22 help -\n",
      "23 i -\n",
      "24 마세요 -\n",
      "25 구매 -\n",
      "26 음식 -\n",
      "27 how -\n",
      "28 love -\n",
      "29 maybe -\n",
      "30 food -\n",
      "31 ate -\n",
      "32 steak -\n",
      "33 flea -\n",
      "34 licks -\n",
      "35 has -\n",
      "36 posting -\n",
      "37 buying -\n",
      "38 please. -\n",
      "39 귀여워 -\n",
      "40 도와주세요 -\n",
      "41 가지마 -\n",
      "42 벌레 -\n",
      "43 so -\n",
      "44 너 -\n",
      "45 있어요 -\n"
     ]
    }
   ],
   "source": [
    "voca=cvModel.vocabulary\n",
    "nword=len(voca)\n",
    "ntopic=2\n",
    "ndoc=df.count()\n",
    "print \"단어수: \",nword\n",
    "print \"문서수: \",ndoc\n",
    "for i,v in enumerate(voca):\n",
    "    print i,v,'-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ldaModel = LDA.train(testRdd, k=ntopic, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic=ldaModel.describeTopics(maxTermsPerTopic=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Topic:  0 ([0, 1, 4, 2, 9, 7, 6, 5, 3, 11], [0.07098817032555373, 0.06858400493443259, 0.05815617460168694, 0.04688995768109478, 0.04548597657539295, 0.04466915188161882, 0.03699346120046559, 0.03141133580014032, 0.024474727844560847, 0.021902283763446686])\n",
      "* Topic:  1 ([3, 0, 2, 8, 4, 5, 1, 6, 23, 45], [0.07204185689860139, 0.058113811271157736, 0.04986810713432724, 0.04467590549607236, 0.038723261571330735, 0.03309567139339414, 0.028407770627501796, 0.02757368242953925, 0.0212141372745852, 0.021159754175251993])\n"
     ]
    }
   ],
   "source": [
    "for k in range(ntopic):\n",
    "    print \"* Topic: \", k, topic[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강아지 stupid dog him 우리 worthless stop to my help\n",
      "my 강아지 him 바보같이 dog to stupid stop has 있어요\n"
     ]
    }
   ],
   "source": [
    "for k in range(ntopic):\n",
    "    print \"Topic \",k\n",
    "    for w in topic[k][0]:\n",
    "        print voca[w],\n",
    "    print \"\\n-----\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 주제별 단어 표를 출력한다.\n",
    "    * 값이 높은 순서대로 각 토픽에 배정된다.\n",
    "    * 높은 상위 갯수를 출력하면, 중복되는 단어가 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_topic=ldaModel.topicsMatrix()\n",
    "print len(word_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.18871533,  1.81128467])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_topic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강아지 \t\t[ 2.18871533  1.81128467]\n",
      "stupid \t\t[ 2.11458983  0.88541017]\n",
      "him \t\t[ 1.4457165  1.5542835]\n",
      "my \t\t[ 0.75460759  2.24539241]\n",
      "dog \t\t[ 1.79307778  1.20692222]\n",
      "to \t\t[ 0.96847787  1.03152213]\n",
      "stop \t\t[ 1.1405866  0.8594134]\n",
      "worthless \t\t[ 1.37724437  0.62275563]\n",
      "바보같이 \t\t[ 0.60754645  1.39245355]\n",
      "우리 \t\t[ 1.4024288  0.5975712]\n",
      "garbage \t\t[ 0.65627642  0.34372358]\n",
      "help \t\t[ 0.6752937  0.3247063]\n",
      "i \t\t[ 0.34672376  0.65327624]\n",
      "공원 \t\t[ 0.67378558  0.32621442]\n",
      "mr \t\t[ 0.35326772  0.64673228]\n",
      "음식 \t\t[ 0.66867928  0.33132072]\n",
      "not \t\t[ 0.66921986  0.33078014]\n",
      "사랑해 \t\t[ 0.34666332  0.65333668]\n",
      "cute. \t\t[ 0.66594385  0.33405615]\n",
      "take \t\t[ 0.34619105  0.65380895]\n",
      "dalmation \t\t[ 0.35478406  0.64521594]\n",
      "problems. \t\t[ 0.35964272  0.64035728]\n",
      "quit \t\t[ 0.34760316  0.65239684]\n",
      "has \t\t[ 0.33880018  0.66119982]\n",
      "마세요 \t\t[ 0.67309352  0.32690648]\n",
      "구매 \t\t[ 0.67052058  0.32947942]\n",
      "귀여워 \t\t[ 0.66492242  0.33507758]\n",
      "park \t\t[ 0.34055008  0.65944992]\n",
      "love \t\t[ 0.35806845  0.64193155]\n",
      "is \t\t[ 0.67162608  0.32837392]\n",
      "food \t\t[ 0.67065747  0.32934253]\n",
      "ate \t\t[ 0.35078472  0.64921528]\n",
      "steak \t\t[ 0.3559982  0.6440018]\n",
      "flea \t\t[ 0.34752669  0.65247331]\n",
      "licks \t\t[ 0.35583862  0.64416138]\n",
      "posting \t\t[ 0.34818672  0.65181328]\n",
      "buying \t\t[ 0.67306094  0.32693906]\n",
      "please. \t\t[ 0.66486279  0.33513721]\n",
      "도와주세요 \t\t[ 0.34474048  0.65525952]\n",
      "how \t\t[ 0.34347766  0.65652234]\n",
      "가지마 \t\t[ 0.35342669  0.64657331]\n",
      "maybe \t\t[ 0.6648881  0.3351119]\n",
      "벌레 \t\t[ 0.34672772  0.65327228]\n",
      "so \t\t[ 0.34826063  0.65173937]\n",
      "너 \t\t[ 0.34853407  0.65146593]\n",
      "있어요 \t\t[ 0.34049519  0.65950481]\n",
      "\n",
      "강아지 \t\t[ 2.18871533  1.81128467]\n",
      "stupid \t\t[ 2.11458983  0.88541017]\n",
      "him \t\t[ 1.4457165  1.5542835]\n",
      "my \t\t[ 0.75460759  2.24539241]\n",
      "dog \t\t[ 1.79307778  1.20692222]\n",
      "to \t\t[ 0.96847787  1.03152213]\n",
      "stop \t\t[ 1.1405866  0.8594134]\n",
      "worthless \t\t[ 1.37724437  0.62275563]\n",
      "바보같이 \t\t[ 0.60754645  1.39245355]\n",
      "우리 \t\t[ 1.4024288  0.5975712]\n",
      "garbage \t\t[ 0.65627642  0.34372358]\n",
      "help \t\t[ 0.6752937  0.3247063]\n",
      "i \t\t[ 0.34672376  0.65327624]\n",
      "공원 \t\t[ 0.67378558  0.32621442]\n",
      "mr \t\t[ 0.35326772  0.64673228]\n",
      "음식 \t\t[ 0.66867928  0.33132072]\n",
      "not \t\t[ 0.66921986  0.33078014]\n",
      "사랑해 \t\t[ 0.34666332  0.65333668]\n",
      "cute. \t\t[ 0.66594385  0.33405615]\n",
      "take \t\t[ 0.34619105  0.65380895]\n",
      "dalmation \t\t[ 0.35478406  0.64521594]\n",
      "problems. \t\t[ 0.35964272  0.64035728]\n",
      "quit \t\t[ 0.34760316  0.65239684]\n",
      "has \t\t[ 0.33880018  0.66119982]\n",
      "마세요 \t\t[ 0.67309352  0.32690648]\n",
      "구매 \t\t[ 0.67052058  0.32947942]\n",
      "귀여워 \t\t[ 0.66492242  0.33507758]\n",
      "park \t\t[ 0.34055008  0.65944992]\n",
      "love \t\t[ 0.35806845  0.64193155]\n",
      "is \t\t[ 0.67162608  0.32837392]\n",
      "food \t\t[ 0.67065747  0.32934253]\n",
      "ate \t\t[ 0.35078472  0.64921528]\n",
      "steak \t\t[ 0.3559982  0.6440018]\n",
      "flea \t\t[ 0.34752669  0.65247331]\n",
      "licks \t\t[ 0.35583862  0.64416138]\n",
      "posting \t\t[ 0.34818672  0.65181328]\n",
      "buying \t\t[ 0.67306094  0.32693906]\n",
      "please. \t\t[ 0.66486279  0.33513721]\n",
      "도와주세요 \t\t[ 0.34474048  0.65525952]\n",
      "how \t\t[ 0.34347766  0.65652234]\n",
      "가지마 \t\t[ 0.35342669  0.64657331]\n",
      "maybe \t\t[ 0.6648881  0.3351119]\n",
      "벌레 \t\t[ 0.34672772  0.65327228]\n",
      "so \t\t[ 0.34826063  0.65173937]\n",
      "너 \t\t[ 0.34853407  0.65146593]\n",
      "있어요 \t\t[ 0.34049519  0.65950481]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(ntopic):\n",
    "    for w in range(nword):\n",
    "        print voca[w], '\\t\\t',word_topic[w]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.10 추천\n",
    "\n",
    "* ALS Alternating Least Squares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "        (0, 0, 4.0),\n",
    "        (0, 1, 2.0),\n",
    "        (1, 1, 3.0),\n",
    "        (1, 2, 4.0),\n",
    "        (2, 1, 1.0),\n",
    "        (2, 2, 5.0)],\n",
    "        [\"user\", \"item\", \"rating\"])\n",
    "als = pyspark.ml.recommendation.ALS(rank=10, maxIter=5, seed=0)\n",
    "model = als.fit(df)\n",
    "model.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(user=0, item=2, prediction=-0.13807615637779236)\n",
      "Row(user=1, item=0, prediction=2.6258413791656494)\n",
      "Row(user=2, item=0, prediction=-1.5018409490585327)\n"
     ]
    }
   ],
   "source": [
    "test = spark.createDataFrame([(0, 2), (1, 0), (2, 0)], [\"user\", \"item\"])\n",
    "predictions = sorted(model.transform(test).collect(), key=lambda r: r[0])\n",
    "for p in predictions:\n",
    "    print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.11 scikit-learn\n",
    "\n",
    "* scikit-learn은 Python으로 만들어진 기계학습 라이브러리를 제공한다.\n",
    "* Spark는 클러스터를 구성해서 데이터를 처리하는 방식이다. scikit-learn은 하나의 컴퓨터에서 실행한다.\n",
    "* scikit-learn은 sklearn와 같은 명칭이고 Python에서 import해서 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFdCAYAAABGoXXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLFhJSmLBSlISC1aSkliwkpTEgpWkJP8HKreVsVJRD2cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f426027f6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "plt.matshow(digits.images[0], cmap=plt.cm.Greys);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster label:  [0 0 0 1 1 1]\n",
      "Centroid:  [[ 1.  2.]\n",
      " [ 4.  2.]]\n",
      "Predictions:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2],[1, 4],[1, 0],[4, 2],[4, 4],[4, 0]])\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "print \"Cluster label: \", kmeans.labels_\n",
    "print \"Centroid: \", kmeans.cluster_centers_\n",
    "print \"Predictions: \",kmeans.predict([[0, 0], [4, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.12 그래프 분석\n",
    "\n",
    "* 그래프를 분석하려면 GraphFrame을 사용한다.\n",
    "```\n",
    "http://graphframes.github.io/quick-start.html\n",
    "```\n",
    "\n",
    "* 더 해보기\n",
    "    * link prediction - similarities\n",
    "    * communities - clustering\n",
    "    * opinion leader - PageRank\n",
    "    * graph partitioning\n",
    "\n",
    "* 참고 논문: \"Social big data: Recent achievements and new challenges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* spark-defaults.conf 수정\n",
    "    * 여러 jar가 필요한 경우, 컴마로 분리해서 추가한다.\n",
    "```\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.jars.packages=graphframes:graphframes:0.4.0-spark2.0-s_2.11\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  c|       1|\n",
      "|  b|       2|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v = sqlContext.createDataFrame([\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "#e = sqlContext.createDataFrame([\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "# Create a GraphFrame\n",
    "from graphframes import *\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Query: Get in-degree of each vertex.\n",
    "g.inDegrees.show()\n",
    "\n",
    "# Query: Count the number of \"follow\" connections in the graph.\n",
    "g.edges.filter(\"relationship = 'follow'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Run PageRank algorithm, and show results.\n",
    "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "results.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: spark-submit\n",
    "\n",
    "* GraphFrame을 spark-submit으로 실행한다.\n",
    "* spark-defaults.conf에 추가\n",
    "```\n",
    "spark.jars.packages=graphframes:graphframes:0.4.0-spark2.0-s_2.11\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_graphframe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_graphframe.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import pyspark\n",
    "from graphframes import *\n",
    "def doIt():\n",
    "    v = spark.createDataFrame([\n",
    "      (\"a\", \"Alice\", 34),\n",
    "      (\"b\", \"Bob\", 36),\n",
    "      (\"c\", \"Charlie\", 30),\n",
    "    ], [\"id\", \"name\", \"age\"])\n",
    "    # Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "    e = spark.createDataFrame([\n",
    "      (\"a\", \"b\", \"friend\"),\n",
    "      (\"b\", \"c\", \"follow\"),\n",
    "      (\"c\", \"b\", \"follow\"),\n",
    "    ], [\"src\", \"dst\", \"relationship\"])\n",
    "    # Create a GraphFrame\n",
    "    # Query: Get in-degree of each vertex.\n",
    "    g = GraphFrame(v, e)\n",
    "    g.inDegrees.show()\n",
    "    # Query: Count the number of \"follow\" connections in the graph.\n",
    "    g.edges.filter(\"relationship = 'follow'\").count()\n",
    "    # Run PageRank algorithm, and show results.\n",
    "    results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "    results.vertices.select(\"id\", \"pagerank\").show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()  \n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 1487ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/61ms)\n",
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  c|       1|\n",
      "|  b|       2|\n",
      "+---+--------+\n",
      "\n",
      "+---+-------------------+\n",
      "| id|           pagerank|\n",
      "+---+-------------------+\n",
      "|  a|               0.01|\n",
      "|  b| 0.2808611427228327|\n",
      "|  c|0.27995525261339177|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_graphframe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 연습: 더 해보기\n",
    "\n",
    "* todo\n",
    "    * spark 이미지 처리 http://docs.thunder-project.org/spark"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
